{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DemTFshpYLwB"
      },
      "source": [
        "## [Ensemble Learning](https://drive.google.com/file/d/1K6a9HZS5NyDWdhezu0gocbQAEUkbUwPl/view?usp=sharing)\n",
        "Ensemble learning is a technique in machine learning where multiple models, often called \"weak learners,\" are trained to address the same problem and are then combined to achieve better performance than any single model could. This approach leverages the diversity among models to reduce errors due to bias or variance in individual predictions. Ensemble methods can be used for both classification and regression tasks and typically fall into three categories:\n",
        "\n",
        "1. **Bagging (Bootstrap Aggregating)**\n",
        "2. **Boosting**\n",
        "3. **Stacking (Stacked Generalization)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBpr79LsYRjQ"
      },
      "source": [
        "### 1. Bagging (Bootstrap Aggregating)\n",
        "[BaggingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html) | [BaggingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-EEBd1y3Req"
      },
      "source": [
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAngAAAEWCAYAAAD1geqNAAAgAElEQVR4Ae2dV3Pc5tuf36+QD5D3LJOZHOYomdSZHOQ4eZOZ0L2qWf5bllUs2aKsanVZxVYX1XsXJZPqvYsSmxopsbdl75135n52nyUWiw7sLrD4YeaZLQCect1Y4uKN9k+ECQRAAARAAARAAARAIKsI/FNWjQaDAQEQAAEQAAEQAAEQIAgeNgIQAAEQAAEQAAEQyDICELwsCyiGAwIgAAIgAAIgAAIQPGwDIAACIAACIAACIJBlBCB4WRZQDAcEQAAEQAAEQAAEIHjYBkAABEAABEAABEAgywhA8LIsoBgOCIAACIAACIAACEDwsA2AAAiAAAiAAAiAQJYRgOBlWUAxHBAAARAAARAAARCA4GEbAAEQAAEQAAEQAIEsIwDBy7KAYjggAAIgAAIgAAIgAMHDNgACIAACIAACIAACWUYAgpdlAcVwQAAEQAAEQAAEQACCh20ABEAABEAABEAABLKMAAQvywKK4YAACIAACIAACIAABA/bAAiAAAiAAAiAAAhkGQEIXpYFFMMBARAAARAAARAAAQgetgEQCAmBisZXdPbRIVp3bhHN2/8Nbbq4lC49O0G1rR9CQgDDBAEQAIHwEIDghSfWGKkJgdFIhHrv3qX2Q4eoee1aalqzhtoOHKCe27dppKnJZG1nswc7eqm5qJIq8x9T6f6rVH74BlVfLaK217U0NjzqrFLFWo0ddbS9YA39n9X/hf7DvH+tWz7d+D/pwM2/qGegS7G2s7f3ynto/ekm+nLDe/rv81/Rvyx/R78drqdLTzqpf2jcWaVYCwRAAARAwBYBCJ4tXFg4Gwmw1DWtXk3VU6YYlsYVK6jnxg1PEDQ+ekMvtl+iO7/sMyyvjtyk9jd1jtr84+ISXaHTk73/OP+fKe/6ZtvtVTQO0i/76uhf/d9npuWbPz7QndIe221gBRAAARAAAesEIHjWWWHJLCMwUFxM9b/+aih1WtJXN3cu9T1+7IhGc1EFPVp13FDqtKSveHcB9TW2W25zzr6vbMudUvpWnf7ZclsL99WaSp2W+H28ppJaukYst4MFQQAEQAAErBOA4FlnhSWziED/s2e2xU4te5z5szM1PHhlW+yUsnd30X7qqWs1bXLD+cWu5E6K3pHbO03bmrr5gyO5k8L3n38qp9Zu94eiTTuKBUAABEAgZAQgeCELOIZLNNbRQTXff+9a8Fj4huusHT7tqmp2JXdS9J5vPmcYwqqWCk/kjiXvf+T+OxoeHdZt749zTa7kTkrep+sqddvADBAAARAAAWcEIHjOuGGtABPoOH7cE7ljwWvdu9cSCb6AQkqa29f6++W6be6/sdUzwWPJu1Z8UbOtkdEJ+ufPXngieCx6d8pwTp4maHwJAiAAAg4JQPAcgsNqwSVQ//PPnglezcyZpiBG+gc9kzuWw+Jdf+u2uejITE8Fb0fhWs228p90eiZ3LHgL91nLhGp2Bl+CAAiAAAgkEYDgJSHBF9lMYHxw0DO5k+fk8e1VjKbuam8Oz8rM3/2lh3Wb+/Xwd54KHt9iRWv646w3h2flYdr/veydVjP4DgRAAARAwCEBCJ5DcFgtmAQmRka8F7x246tbu6tbPM3gGQne3mubPBW8whfa5/xtvdjsaQYvZ1VFMDco9BoEQAAEfEoAgufTwKBbqSPQ8Ntvnkle3Zw5ph0dHRz2VPBK9hTqtlnR9Nozwftvv/wbGhjq02zrZkm3p4K38liDZjv4EgRAAARAwBkBCJ4zblgrwAS68vM9Ezy+YMPKVH7ohmeS1/j4jWGTq88s8ETy9t3YYtjOv59Z6pnklVUPGLaFmSAAAiAAAvYIQPDs8cLS2UBgfJwacnNdSx5n78Z6rF39yfevk+fQuXl9sS3fUgS+35njSvKWHJtl2s7x222eCN78vbWmbQVtgUhBLuXk5ChKLhUan6qpGGIZ5eXkUF6J4iuztyV5lJOTR2Vmy9mcL8axqJB0ux4ppNyEcU6OObdAdy3TXpTtscPLtDosAAKhJADBC2XYMejhqirX98IbfPXKFsimZ+9cSd6DZUeov6XTUpsTExP0+6n5jiTvz8u/W2qDF1p5vMGV5H2yJtvugRehwkUsOSrZEgKWQ26kx3JQPFzQquAly2hUUnP2OFBOwQqC52EYUVVICUDwQhp4DJtopKmJWrZutZ3Ja96wgYaqqhwhbC2rpqcbz9gWvfKD12mw3Vq2UNmx901vyMqTLf5l1X8ivmK2udP+uXD7rrbSv/222LboLT1Sr+xqVrwv25NDOXoZr1i2K1mG/Dt054JHRE7HC8Hz7waBngWKAAQvUOFCZ1NBYOj9e2o/dMhU9Nr27aPBN8bnv1ntX8vL91S6z/zmx+/O3Ce+CteLqbjqKR26vZ2WHv+RZu74f7Ty1Dw6cT+P3tTbORao35NDN1rpfy19Zyh6/3XuK9pwpolau7Lw8WQWhEYtgEKg9hTGsn45FM14aRyijWUAo4d9c6mwgA/JKrJcYr7MGkaziLkFZZP18mFULfFMqJczj4o62dH4ULPWenIzMBmzerxiNYM2RXuKQ77KjKeoSzHPsF+yf3gFgRATgOCFOPgYejIBPnTbc+sW8cUT7UePUs+NGzRUmbrDiKMDw9T2qoaqr72g18dvEwtd/b0y4kebBXXq7BujguddtP5ME33/VzUtP9pAp+61U3XLUFCHZK3fCZKls4pqGSk0SpEhUgmeWEchXnFBUn+XKHgsg/FsYUzEEtpR10tEUYmS9bgXvOj4JusjC23qLpMgmi4OAeuEBl+DQLYRgOBlW0QxHhAAgYwQSJIZrV6oBEd7HaXgyWxc4gULUREzETzV+W/qbJr4rFpGLVeifwlipRqUSQYvWt+k4FlpU92HJOGNdUE9HlXP8BEEQk8Aghf6TQAAQAAEvCCgLWuqmrUEL0mgFIKnJ1CqehJFykAKk9qK9k/IUvzw56Q4ei14Shp6bSYLXsJa4grj+BXKOuNRroH3IBBWAhC8sEYe4wYBEPCWgJCuyWyVZuWqZbQFKk2CJ/oib2sSkzqVOGr3TzEyPQGNLZIkvRba1BK8BBmMSR0yeIo44C0IaBCA4GlAwVcgAAIgYJuAiexwfWop0RaodAhe7HYuKT5EK8Ybb8Nam0mCp8NVzdJ2vLACCGQ5AQhelgcYw3NGYGRkhIaHh52t7GCt8YkJ6uwfpd6hMQdr21hlYpxouIlopNXGSvYXHR8fp6amJopEEs8ds19TsNYwlA4NUTEVPDI43Kq84jUhM2iwTvyQpkIiFYhF/xX1avdPsYLGmOJzk+ZZazNJ8BLGJmuP1oUraSUPvIJAMgEIXjITfBNCAkNDQ9TW1ka1tbX09u3bhFJTU0Otra00ODjoKZnGrmF6XNVD51+2Ud795oRSUNZBxfV91DXgwe1EBt4SRU4S1awgejslsdStJ2q/RDTc6HpsFRUVdOrUKVqxYgVNmTIloaxbt47y8/Opvj777n2XCC6WpbJ4o2NtgVKJkOqwaVSAVDdTTpAgK4IX62dc+Igm6528+la7f4oRJ0mcnBcTsHj2jr+31maS4MXaUF4BHBVRFQPZNF5BAAQEAQgeNoRQE+jt7aW6uroEoVMLnvIzy16PxceT6YF92zxAhx+3JAidWvCUn6+96iSWQdtT932i6mWJQqcWPOXn+s1E/faezsF9evjwoabUqSVPfv7jjz+otLTU9nCCtIIQo/hFC8n3l5Nj0RYoleDxwkLg5PlyeVSWIHRyvjz/z4rgcaUxCYv3M4/KVDKl3T/Z+8mbGccveojXpffUDvM24yLIdUlBTBh/9Pso48kLQhS9wlsQAAEiguBhMwgtgebmZstip5Q8ft/YaD/jNTI2QYXlHZbFTil5/P5Zda+1WE2MErGsKeXNzvvIKUvt8OPQtm3blpCpkxJn5fXYsWOW2sFCyQRMxSt5FXwDAiAQMgIQvJAFHMONEmhpaXEsd1L2+BwzO5MbuZOy96K2z7zJ+i3O5U6KYOs503bcyJ0UwJMnT5q2E+4FtA51Rr9THrIMNyOMHgRAQIsABE+LCr7LagJ9fX2u5U5KXnd3tyVWJfV9jjN3Uu7ka1vviH6bHdfcy52UvAH9J3jcunXLceZOyp18ff36tf54MCf+TFflYVDIHTYMEAABMwIQPDNCmJ91BPhEfylobl/5nDwr04lnrZ4J3oNKA6msWuSd4DXl6Q5tyZIlngnezp07ddvBDBAAARAAAWcEIHjOuGGtABNwK3Xq9fmWIEYT3/5EZt+8eD39XOcWJyMR7+SOs3jvf9YcVmdnp2dyx1m82bNna7aDL0EABEAABJwTgOA5Z4c1A0hgbGzMs+ydFD2z++U1dA17KngHHrZokx+o8Fbw3k7VbKe6utpTwWPJ4/sOYgIBEAABEPCOAATPO5aoKSAEpJh59crSaDR1eJzBO6WbwWvxVvDez9McVkdHh6eC9+OPP2q2gy9BAARAAAScE4DgOWeHNQNKIBPn4B1/GvEsi3evwuAcvA8LvZO8pj26EV68eLFnkrd9+3bddjADBEAABEDAGQEInjNuWCvABPjmxl5l77q6uiyReFnn3VW0kR6Dw5kdhd4JHj8BQ2e6fv26Z4JXVlam0wq+BgEQAAEQcEoAgueUHNYLNAE3NzmWctjQ0GCLweVS5zc5lhdnPK+xcLPjuo3uJY8fbWYybdmyxbXkHT161KQVzAYBEAABEHBCAILnhBrWyQoCfKNiKWt2X+3KHQMbHBmny6Xtjg/VPqnqscZ9fICInzEr72dn97XliKV2+OKSzZs3O5a8gwcPWmoHC4EACIAACNgnAMGzzwxrZBEBfq4s38vOquBVVlaS1cOyepjKG/uJr4SVWTmz14KyDqrrGNKrTv/7zptEFTOti17tOqLeF/r16cy5ffs2zZo1y7LorVmzhoqKinRqw9cgAAIgAAJeEIDgeUERdQSewMDAAEUiEeJbgKhlj7/jR5v19/d7Ok6WNr5p8emi5Jsg55e0U1FNL7X3jbpvs6+UqOUQUVVusuzVrCRqPUs0WOW6nfLycjpy5AhpXYCxcuVKOnPmDH348MF1O6gABEAABEDAnAAEz5wRlggZgYmJCRoaGhLF7CbGXqEZHp2g1t4R4psiT0x4VatGPXz4drCaaKieaML49i4aa1v+anBwUGRG+Ypls9vIWK4UC4IACIAACFgmAMGzjAoLggAIgAAIgAAIgEAwCEDwghEn9BIEQAAEQAAEQAAELBOA4FlGhQVBAARAAARAAARAIBgEIHjBiBN6CQIgAAIgAAIgAAKWCUDwLKPCgiAAAiAAAiAAAiAQDAIQvGDECb0EARAAARAAARAAAcsEIHiWUWFBEAABEAABEAABEAgGAQheMOKEXoIACIAACIAACICAZQIQPMuosCAIgAAIgAAIgAAIBIMABC8YcUIvQQAEQAAEQAAEQMAyAQieZVRYEARAAARAAARAAASCQQCCF4w4oZcgAAIgAAIgAAIgYJkABM8yKiwIAv4i8Pz5c5oyZYq/OoXegAAIgAAI+IIABM8XYUAnQMA+AQiefWZYAwRAAATCQgCCF5ZIY5xZR+DZs2fI4GVdVDEgEAABEPCGAATPG46oBQTSToAFb+rUqWlvFw2CAAiAAAj4nwAEz/8xQg9BQJMABE8TC74EARAAARAgIggeNgMQCCgBCF5AA4dugwAIgEAaCEDw0gAZTYBAKgg8ffoUh2hTARZ1ggAIgEAWEIDgZUEQMYRwEmDBmzZtWjgHj1GDAAiAAAgYEoDgGeLBTBDwLwEInn9jg56BAAiAQKYJQPAyHQG0DwIOCUDwHILDaiAAAiAQAgIQvBAEGUPMTgJPnjzBIdrsDC1GBQIgAAKuCUDwXCNEBSCQGQIseNOnT89M42gVBEAABEDA1wQgeL4ODzoHAvoEIHj6bDAHBEAABMJOAIIX9i0A4w8sAQheYEOHjoMACIBAyglA8FKOGA2AQGoIPH78GIdoU4MWtYIACIBA4AlA8AIfQgwgrARY8GbMmBHW4WPcIAACIAACBgQgeAZwMAsE/EwAgufn6KBvIAACIJBZAhC8zPJH6yDgmAAEzzE6rAgCIAACWU8Agpf1IcYAs5XAo0ePcIg2W4OLcYEACICASwIQPJcAsToIZIoAC953332XqebRLgiAAAiAgI8JQPB8HBx0DQSMCEDwjOhgHgiAAAiEmwAEL9zxx+gDTACCF+DgoesgAAIgkGICELwUA0b1IJAqAg8fPsQh2lTBRb0gAAIgEHACELyABxDdDy8BCF54Y4+RgwAIgIAZAQieGSHMBwGbBCYmJmh8fFyUsbExGh0dFWVkZISGh4dFGRoaosHBQRoYGBClv7+f+vr6ROnt7aWenh7q7u4Wpaurizo7O0Xp6Oig9vZ2Ua5cuSKeZBGJRIhLS0sLNTc3i9LU1ESNjY3U0NAgSn19PdXV1YlSW1tLNTU1olRXV1NVVRV9+PBBlPfv31NlZaUoFRUV9O7dO3r79q0ob968odevX4vy6tUrKi8vF6WsrIxKS0uppKRElOLiYnr58qUoL168oKKiIlGeP39Oz549o6dPn4rCj1rjW71w4cPNLKwPHjwQ5f79+3Tv3j1R7t69S3fu3BHl9u3bdOvWLbp586YoN27coOvXr4ty7do1unr1KjEXLoWFhVRQUCDK33//TZcvX6ZLly6Jkp+fTxcvXhTlwoULdP78eVHOnTtHZ8+epTNnzohy+vRpOnXqlCgnT56kEydOiHL8+HE6duwYHT16VJQjR47Q4cOHRTl06BAdPHiQDhw4IMr+/ftp3759ouTl5dHevXtF2bNnD+3evZt27dolys6dO2nHjh20evVqYtaYQAAEQMANAQieG3oBWZd3Ftu3bxdl27Zt9Ndff4ny559/0tatW2nLli2ibN68mTZt2iTKH3/8QRs3bqQNGzaIsn79elq3bp0oa9eupTVr1ojCO6NVq1bR77//LsrKlStpxYoVoixfvpyWLVsmytKlS2nJkiX022+/ibJ48WLKzc0VZdGiRfTrr7/SL7/8IsrChQtpwYIFovz88880f/58UebNm0dz586lOXPmiPLTTz/R7NmzRfnxxx9p1qxZ9MMPP4jyj3/8g77//ntRZs6cKQ5l8hWn/OSH6dOn07Rp00SZOnUqTZkyRZRvv/2WvvnmG/r6669F+eqrr+jLL78U5YsvvqDPP/9clM8++4w+/fRT+uSTT0T5+OOP6aOPPhIlJyeHsrHI8fFY5biZAbOQXJiR5MXsmCPz5MJsJWdmLvlzLDgmHBsuHCsZN46hjCfHlmMs482xl9sBbxO8bcjthLcZuf3wtiS3K97GeFuT2x1vg3J75G2Tt1G5vfK2K7dj3qbl9s3bOm/zcvvn34L8XfBvhH8v/Lvhwr8h+Xvi35b8nfFvjn978nfIv0n5+2TB4zHyKyYQAAEQcEMAgueGXkDW5UwM78A4Q8DZAs4acPZAZhI4qyAzDJxtkJkHzkJwNkJmJjhLITMWnL3gLIbMaHB2Q2Y6OOshMyCcDeGsiMyQcLZEZk44i8IZFc6scOEsi8y4cPZFZmI4K8PZGZmp4ayNzOBwNoezOjLDw9kemfnhLJDMCHF2iLNEMmPE2SOZSeKsEmeXZKaJs04yA8XZKJmZ4iwVZ6tk5oqzWDKjxRLNmS7OeHFh5jITxlkxmSHjbBlnzWQGjbNpMrPGWTbOuHHmjdthPjIjx9k5malrbW2ltra2eCaPs3oyw8fZPpn54ywgZwNlZpCzhDJjyNlDziLKjCJnF2WmkbOOMgPJ2UhM6SXAcse/VUwgAAIg4IYABM8NvYCsy0LBWSXeoWMKBgEWTs54YQofAc7mQfDCF3eMGAS8JgDB85qoD+uD4PkwKCZdguCZAMri2RC8LA4uhgYCaSQAwUsj7Ew1JQWPD8dhCgYBPmyMDF4wYuV1LyF4XhNFfSAQTgIQvBDEnc/74kO0ELzgBJsFjy9GwBQ+Aix4fK4sJhAAARBwQwCC54ZeQNaF4AUkUIpuQvAUMEL2lq+qheCFLOgYLgikgAAELwVQ/VYlBM9vETHvD1/hiwyeOadsXAKCl41RxZhAIP0EIHjpZ572FqXg8a0wMAWDAAQvGHFKRS8heKmgijpBIHwEIHghiDkEL3hBZsHjmwFjCh8BFjy+VyUmEAABEHBDAILnhl5A1uWb7PJFFsjgBSRgROImzBC84MTLy57yEy4geF4SRV0gEE4CELwQxB2CF7wg85M2IHjBi5sXPYbgeUERdYAACEDwQrANSMHjR1FhCgYBCF4w4uRFL/kRespJS/DUyyiXx3sQAAEQ0CIAwdOikmXfQfCCF1AIXvBi5rTH/PxgpcCx4PGzouXE83gZTCAAAiBghwAEzw6tgCzLD7BXTvywez4HDxk8JRV/vecLYZQTC9706dOVX+F9FhNYt25dXPL+/PPPuOCx3K1duzaLR46hgQAIpIoABC9VZDNc76lTp+I90BK8/fv3x+fjjT8IHD58ON6Rx48fJwnegQMH4vPxJrsI3L9/X/wTxkInBY/f8z9m9+7dy67BYjQgAAJpIQDBSwvm9Ddy7do1Ysnr7u4mteCtXLmS3r17l/5OoUVDAhcvXqSjR4+KZdSCt3z5cqqpqTFcHzODS2BsbIw+++wzIXRLliwh/o2y3PF3PA8TCIAACNglAMGzSywgyw8ODoodBEueFDzeUWzcuJFmz54dkFGEq5ttbW3xmEnBq6uro++++44WLFgQLhghHO3OnTtF/FnsZOHn0mICARAAAScEIHhOqAVkHT7U89VXXxHvJHiHwZ+/+OILOnPmTEBGEL5u/v777yJWmzZtEo8q+/7778XngoKC8MEI2YhLSkroo48+issdvy8uLg4ZBQwXBEDAKwIQPK9I+rCe0tJSsbNgqWPBk69NTU0+7C26xARu375Nn3/+uYjXxx9/HI8ZZ2QxZT+BGTNmxAUPF9lkf7wxQhBIJQEIXirp+qBuPrzHcvfll1+K18WLF/ugV+iCHoGhoSFisZNCzlLOh9UxhYMAn4PJMeeivOgmHKPHKEEABLwkAMHzkqYP6zp79mw8I8QnbF+9etWHvUSXlAT4PmgseLIUFRUpZ+N9FhOQ96zk2PN7TCAAAiDglAAEzym5gKzX2NgYFwXeafT29gak5+Ht5vPnz+mTTz4RceNz8DCFi8C8efNo7ty54Ro0RgsCIOA5AQie50j9V+GKFSuELGzYsMF/nUOPNAlMmTJFHKY7duyY5nx8mb0E8vPziW+ZgwkEQAAE3BCA4LmhF5B179y5IwSPb5yKKRgEDh48KGJWX18fjA6jl54RaG1tpUgk4ll9qAgEQCCcBCB4IYj7yMiIuF1KCIaaNUPkG1Fz5hUTCIAACIAACDghAMFzQI0zYXwD4SCVX3/9NTD9vX79uoOoZN8qfMsUTO4JdHR0BGbbD9LfFGVf3UcJNYAACHhNAILngOiyZcsCt8PYtm1bYPrMF4OkalLulPz+/vjx44GJmWRZXl6eqtA5rpf7FsTfrGTq91f+vfox7o43GKwIAllCAILnIJByZ+FgVaxiQuDVq1fi3DOTxRzP5p2R33eYQe4f/zb8NjFPP/bLb5yc9geC55Qc1gOB1BKA4DngC8FzAM3iKukQPGQbLAbD5mJ+FSm/9ssmXt8uDsHzbWjQsZATgOA52AAgeA6gWVwFgmcRlA8X86tI+bVfPgyhoy5B8Bxhw0ogkHICEDwHiL0SvPEUP1800jNCpQ39dPtdF10sbqfCsg56UtVDVa2DNDQ67mDkqV8Fgpd6xqlqwa8i5VW/+DFyqZz4YpAHDx7QkSNHaN26dbR69Wrat28f3bhxg/x8uxwIXiq3CtQNAs4JQPAcsHMqeANlZdRx4gQ1rVpFNTNmUPWUKVT7ww/UsmkTdV26RMM1NQ56k7jKwPA4FdX00slnrZR3v9mwXH/dSbXtqd1pJfbO/FMQBG9iYoLGxsbMB+NwifGJCarrGKLnNb10pbyTLrxsoxtvuuhlXR81dg07rDX1q3klUl731Gm/GhoaqLCwkLZu3UqzZs0ivvn09OnTacGCBcTPjH358qUnXeWr8tevXy/q5zb0ypIlS0R/+LZHfpogeH6KBvoCApMEIHiTLCy/syN4E0ND1JWfL2SOhc6ssPz1PXxouS/KBVkAzKROa35BWQd1Dowqq8rYez8KXn9/v8is7N27VzxCSu6A58yZQ7t27SK+kXR3d7drZi09I3S3ottSDB996KHOfn/ETA7cqUjJ9VP1ardf/Kg4K8Ilt4MzZ844egTg27dvaenSpbpCJ+tXv7Js8jbnlwmC55dIoB8gkEgAgpfIw9Inq4I3UFpKtbNnm0qdlvQ1r19PYzakgQ+9asmb1e+OP434Qhj8JHi1tbXiEJl6B6v3mWWvsrLS0jakXGhkbIJuve1yFL+H73uUVWX0vV2RSldnrfZreHiY/vzzT9vCxdvDjBkz6MmTJ5aHxBf66G1HVr+/cuWK5fZSuSAEL5V0UTcIOCcAwXPAzorgDb5540jslLLXuHIlTVg4FFgZGXQkB2r5u1TS7oCGt6v4RfD4WaBWd7Tq5U6ePGkZyuj4BOWXtLuK3403nZbbS+WCVkUqlX3QqttqvzZu3Og45nIbKCoq0upCwnd8Lt/8+fNdt8VtchYw0xMEL9MRQPsgoE0AgqfNxfBbK4LX8NtvrgWPZa/96FHDvvDMs0VtrgRBKXrvI4Om7aVyAT8Inhu5kzv6EydOWMJ0552zzJ0yZvyez7vM9GRVpNLdTyv9unDhgifCNXfuXDI7Ry4/P9+Ttnhb27JlS7pxJrUHwUtCgi9AwBcEIHgOwmAmeH2PHnkidzKbN9ajfxiurXfEM7ljUbj5pssBEe9WybTgVVRUeLbzNTsJv9Xj2GX6ymgrIuXdlmK9JrN+8UUzP/zwg2dxv3r1qmHnli9f7llbLHkDAwOG7X7x5pQAACAASURBVKV6JgQv1YRRPwg4IwDBc8DNTPBa9+71VPD6nj7V7eX7Vm8Oz8qM0JmiNt220jEj04LH59HJLJzbVz7kZzS9qO31VM75UH0mJzORylTfzPr1+vVrz2LO28zmzZsNh8pX4rrdtpTrV1VVGbaX6pkQvFQTRv0g4IwABM8BNzPBa1q71lPB6y4o0O1lVZu3gnf2RbgFT94OQ7kDdfPe6HDdTYcXVkgZV7/ybVUyOZmJVKb6Ztav+/fveypcubm5hkPlCzLcbFPqdWs8uL2SYYdNZkLwTABhNghkiAAEzwF4M8GL/PWXp4LXe/eubi+7BkY9zQLxbToyOWUyg9fX1+fpjpd3xM3Nzbo4+QbUaklz85kzgpmczEQqU30z69eLFy88jTvfoNhoWrt2rWftTZs2LaX3ZDQah5wHwZMk8AoC/iIAwXMQDzPB44ybPH/Oi9fhujrDXl4u7fBMFPgGu5mcMil4fC6WOjvi9nOPwfmTpfXO7luoJ4HVbZmNnZlIZWq7MutXZ2enp3Hn9oymmzdvetYe35sx0xMEL9MRQPsgoE0AgqfNxfBbM8Eb6+qi6mnTPJG8ZpPzuLijDZ3Dnghepi+w4LFkUvC4fS+zK4sXLzbcjnoGxzyJGwvf0ScRw7bSMdNMpNLRB602rPRr586dnkmXlceKrVq1ynV7fKjXKEOsxSIV30HwUkEVdYKAewIQPAcMzQSPq+y+etUTwRuurrbUw1eN/a5kge+BxzfczfSUacHz8nwsKzeifVbtzYUWr5syeyUlbzdWRCoT25eVfkUikfjjyNxkbc+fP29piCxm/CQUN23xEzf8MEHw/BAF9AEEkglA8JKZmH5jRfC4ks6zZ51L3tSp1G/hpqnKzn5oHaTTRebPoFUf4uOnIfDzT91M44ODxDLKhd87nTIteNxvp08zUO6sORNodbrt8mILfoqJHyYrIpWJflrtF19N6+Z2Kfx8WjtTR0cHOckccvbvzZs3dprSXJYlk2+U3N7u7gbnEDxNvPgSBDJOAILnIARWBY+r7n/xguze9Lh1924aaWhw0LPoKix61153Gmb0WAT5qks+TOh04qd1tB87pjk+HjPfpHnw9Wtb1ftB8PiRVW6easD3Oevqsnc/wRe19s/HO/y4hThz63Ya6+ykoXfvaKSx0VVVVkXKVSMOVrbTr9bWVjpw4ICtzNqCBQvo0aNHDnoWXYUfiXfs2DGaN2+eYbu7d++m0tJSx+20tbVRYWEhbdiwQbOd7du3071794i3fzsTBM8OLSwLAukjAMFzwNqO4MnqB/k/5SNHqP6XXzSzek2//05d+fk02tIiV3H9yo/B4vPz+GT++5XdxIcDKyIDxFfeupl6HzygurlzNcehdVFJ3Zw51Hv/vqUm/SB4sqMFBQU0depUzZ2hMlunfM9PRBgfH5dV2HodHp2g0oZ+ulhs/Oiyv0s76E2zi0Oy4+PEN+Nu3bVLM4bNa9dS16VLNNLUZKv/dkTKVsUuF3bSL86ucfzXrFmjG38WwbKyMpe9S1ydZe/OnTvET0I5fPgwXbt2jd69e5e4kM1PfBsVu/d35Mft9fZauyobgmczIFgcBNJEAILnALQTwVM2M9bRQQPl5dT38CENVVbS+FBmr35U9s3sfWTHDk0p0BI79Xe8rtnkJ8GTfeXsDGc3lCKnfL9161aR+RgddSfOsj1+7Rsao6rWQfEIMr51zcu6PqppH6LBEWfyyHVODA1R57lztuLHt/wZqqhQdk33vROR0q3Mwxlu+8X3Mvzw4QM9fvxYZNA4ExaU6datW7rbrXIb1nr/008/WXrWLQQvKFsD+hk2AhA8BxF3K3gOmvTFKnzoWC1tdj9z1sho8qPgKfvb2NhIfN80PsHdytWSynUz+Z7PjaxfuNBx/Ixuti3H5VakZD1ev/q1X16PU13fkydPHMudFD4+J5G3eaMJgmdEB/NAIHMEIHgO2IdR8Lx8vi5nLvUmvwueXr/9/D3ftqd+/nzHciclvuf2bcNhWhKpSCHl5uQQS0HOokLSu7lLpCA3ukxODuUW6C5FhYtideXkUqHOYpb6ZTiy4M0cGxujuXPnuhY8Fj3OUBtNEDwjOpgHApkjAMFzwD6Mgte8bp1rQZCi0LRmjS51CJ4uGsczvHo2cs1339FYt/6TTiyJFATPcRztrMjn8cksnBevRtlqCJ6dyGBZEEgfAQieA9aOBW+4haj/DVHP0+jriE7KwUGfUrnKxOioZ3InJW9iZESzyxA8TSyOvxQ33Z4yxbP4df39t25fsk7wRruJBj4Q9b4g6islGqojGg/G+bJ79uzxVPBuG2RvIXi6PwnMAIGMEoDgOcBvS/AG3hN1XCVq2KFfOq4RDVY56El6Vhnt6PBMEKTgjeqcqO5HwRsdGKaOigZqePCKam+VUGtpNQ20++Pec2ZbQP/z557GrmXzZt0mLQme7tqpm2GrXyNtRN0PiVqO6f9eW88R9RYTjTu/32PqRhut2c1tfrQyfhcvXtTtMgRPFw1mgEBGCUDwHOC3JHj9b4maD+vvJLSEr+Uo0YC1KxYddNv5KuPjnkoCS97EmPb99/wieCx19ffL6eXOy3Tnl32a5cn60/T+8hPqbfDvVZU9N296GrvGZct0tyNbIqVbi/czLPVrpJ2ovdDe75V/w92Pve+wBzXm5eV5msG7e/eubq8geLpoMAMEMkoAgucAv6ng8R99LYGz+l3PMwe9SlyF74HX0jNClZFBqu0YcnVDY66Zn4krs29uX5s3bEjsrOKTHwSv6spzTaHTEz3+vuzANeptdPdEAAUGz97y01Dcxku5fovBCfeWRMqzkVmvyLRfQ/VEjbud/2ZbzxNN2Ls5sFbvOwdGqbp9iPhG5a29I+Tm4TIPHjzwVPCMnnkLwdOKJr4DgcwTgOA5iIGh4PWVOd9RKAWQz9WzOfUOjVFJfR/xc2XVjyOTn/meanUd9s8j8lIU+p/pC2wmBW98ZJSKdxfYljspfncX7aeOd/U2o5a8eP/wuLhB9buWAWrqGqahUef3vhvv7fVU8PgZy3qTqUjprZji7w37NdZH1LTf/W+Ws38OJpa5m2+6dH+vheUd4mklww62gYULF3oieTtM7l8JwXMQeKwCAmkgAMFzAFlX8CZGiZry3O8sWPSaDlruGT9u7F5Ft+5OQsqd8vXE01aqaLH3NIS2gwddy0LbgQOG48qk4L06ctOx3EnJe7DsCA12WHsCgBJEc/cIPfrQQ6efaz9L+PzLNvEkkvY++zdTbjt0yHXcOItXO2uW4XOGDUVKOdg0vzfsV+dtb36v/JsdqLQ8svLGfjr4sMXWb5afOWxH9F6+fOla8PjxaWbPqoXgWQ47FgSBtBKA4DnArSt4nHVTZuHcvucLNEymxq5hOvTI3o5CKXoP3uvf9kKraRY05SE7O+/b9u/XqjLhu0wJXmtZtWu5k5L39vS9hDEZfeBs6oWX+hlXZazk++uvOynSo30VslZb4wMD1LB4seO4yRgb3b+Q2zUUKa2Opek7/X6NEzXu8u43215gaUQcPxlLu6+nnrcSH8q1OvFTWLQumrDy3S/8WMXqatOmIHimiLAACGSEAATPAXZdweu6593OguWw2/gB5py54wfO291JqJfnZ9TamfpfvKDm9estCwPfQ4/XsTJlSvBeHXafvZOCd+fXfTQ+on0RiZJBcV2fq9jxIVyr00hzMzUuXWo5ZlLq5GvPrVumTemLlOmqKV1At198myK3/4Qp12/cazqOO+/0D8eqf5d6n8+9aKPxCdOm4gtEIhHxXFsrUieXuXTpEvHNkq1MEDwrlLAMCKSfAATPAXNdweu86e0Og4XRYLr11v3OQu5EnBz6G66tpc4LF6hp7dokceDveN5wTY3BCJJnZUrwHq446lkGj0Wvu6YleXCKb941D7iSOxm3+k57J/d3X7mSFCspcVqvbfv20UhDg6Ln+m91RUp/lbTM0e3XcKO3v9fGPYbj4WcJy7i5fbX7Txl3bGBggPhqWD6nbtq0aQmZvZkzZ9KBAweoqKjIcAxaMyF4WlTwHQhkngAEz0EMdAWvp8jbHUZfiW7vBkbGPdtZ8M6Gz+9xO422thIXN1OmBI/PnYtn4HRui2Jnfmel/vM7x8Yn6MjjiCfxu/DS2S1aBkpKqP3wYWr47bdE4Zs6lSJ//UWcsRvrsbdN6IqUmw3Cg3V1+zXW6+3vNXLasLduDs2qhfDoE/c3Se/t7RXPUx4asn/RlXKgEDwlDbwHAf8QgOA5iIWu4I20ervDGO3U7V2th9kA3nnkl/jjFh+ZErwX2/I9FbxBgxshv/Uoeyd3+nweppuJnyrCWTqjx5BZqV9XpKysnMJlDPvFNy1WHmZ1856fUGMwHbB5UYWMr95rZ7/1c/EMuuV6FgTPNUJUAAIpIQDBc4BVV/C4Ln4qhZudhFy30/icJ76/nd4ffiffny5yl3lzgFFzlUwJXs2NYs8Er+gv/bv+86DtXvFsFk++NY4fJkORymAHDfvFFzLJ35yb16Z9pk+22Peg2dPfLN/n0g8TBM8PUUAfQCCZAAQvmYnpN4aCx8+qjJx0t9OInCHiW64YTE3dw57uLK6Udxi0lr5ZmRK80cFherjymCeSFyn+YAjshourKLVk76nNi2QMO+dipqFIuajX7aqm/eJHk7mRO17XwhXverfA0Yqple/6hq1dBOGWn9n6EDwzQpgPApkhAMFzwN1Q8Lg+vnlq20VnO422y6aZANllr87j4p1JWUO/rDajr5kSPB5026sa14L3/pL5o6v4fEcrO3Cry7xu8kfsTEUqQ1uWpX7x4VUnktd8kGjQWOjlsPk+h1ZjarbchWJ/nFLBY4PgyQjjFQT8RQCC5yAepoIn6xysImJhs7LjaP+baNDeFacvXd5mQ+5Ejj2NEJ/474cpk4LH42fJe7z6hCPR40ecWZnqO7y7mpJj2GXjvmhW+ud0GUsi5bRyF+tZ7tdoF1HXfWu/15bjRL3FRGT9KSMcJ/mbc/vKp2j4ZYLg+SUS6AcIJBKA4CXysPTJsuDJ2vhqvf7XRHwbFT58y49G4lc+z45vjswZP4fTlVfOb5oqdzJ8+wa/TJkWPOYwMTFB9ffK6Nmmc5ZEr/LCI+qPdNlCeLm0w5OdPd9XzS+TZZFKc4cd9Yv/2eL7ULaeJWo+RNRylIgfR9ZbRDRsfAsco+G9aux3Hff7lfZuTm7UHy/mQfC8oIg6QMB7AhA8B0xtC56DNqyuwom3G2+cSR4/AYOfhemnyQ+Cp+TRU9dKtbdLqPzwDXq64YzI7vEVtx/+fkrtr2vZBpWLW37P9x108wQSlvMzRW2unlNrubMWF3QkUhbrdrOY3/rFjymT/1zZfX343t6ta9xws7ouBM8qKSwHAuklAMFzwNtPgie7z1k4fjC51R3G85peX8mBHIffBE/2KxWvfBUkP3rKasyUy/FtbfhJJn6a/CZSko0f+8Wxs3NO3o03XcQXVvlxguD5MSroEwgQQfAcbAV+FDw5jO6BMXHBBN9UlW+GylLA99/KL26nxx96qLbDP4djZZ+Vr2ESPDluvsDlTJE10btY3E52HlEm20jHqx9Fisft135x3/jcV86i82HXk89aaX/sVip8Xiw/qeZN0wD1D1s/zy8dcVa3AcFTE8FnEPAHAQiegzj4WfDUw/HLxRPqful9DqPgSRacoeHM6t+lHfHDt4cfR4hvYcMX1LT2+uO+Z7K/6le/ipRf+6XmJz87POovV0/7KwQv7cjRIAhYIgDBs4QpcaEgCV5iz/3/KcyCp47OyJiz8/vU9aTrs19Fyq/9SldcUt0OBC/VhFE/CDgjAMFzwA2C5wCaxVUgeBZB+XAxv4qUX/vlwxA66hIEzxE2rAQCKScAwXOAGILnAJrFVSB4FkH5cDG/ipRf++XDEDrqEgTPETasBAIpJwDBc4BYCh7vOFC8Z8A7jFRN2Bmliqx/L2aQgoffqve/VWaK31TqflOoGQTcEIDgOaD36NEjiF0K5fb69esOomJtFd4ZYUefmh29FClrkUjfUh0dHYh5Cn+vHHdMIAAC/iMAwfNfTNCjFBKA3KVO7phteXl5CqOHqkEABEAABKwSgOBZJYXlQAAEQAAEQAAEQCAgBCB4AQkUugkCIAACIAACIAACVglA8KySwnIgAAIgAAIgAAIgEBACELyABArdBAEQAAEQAAEQAAGrBCB4VklhORAAARAAARAAARAICAEIXkAChW6CAAiAAAiAAAiAgFUCEDyrpLAcCIAACIAACIAACASEAAQvIIFCN0EABEAABEAABEDAKgEInlVSWA4EQAAEQAAEQAAEAkIAgheQQKGbIAACIAACIAACIGCVAATPKiksBwIgAAIgAAIgAAIBIQDBC0ig0E0QAAEQAAEQAAEQsEoAgmeVFJYDARAAARAAARAAgYAQgOAFJFDoJgiAAAiAAAiAAAhYJQDBs0rKg+UiBbmUk5NDuQUR/dpK8sQyeSX6i1idU7Ynh3IWFZJBa0lViT5aXidChYtyyIu+JnUEX4AACIAACIAACDgmAMFzjM7+ilHBy6VcXSkqo7ycnMAInhDInPAIXnoFPSrPOXvKbGxo0XUM/4EgIjkO/mdDFFtt2OhOVi0qf5t5pB+RWMws/4NkAEj8o5dLhXb+OxPrGPWP24v1UcY+x2x5gz5iFgiAgK8JQPDSGJ7ojjWP8jizprFT5fm5e/Io1yNpSl0GT+7svJPRNIbBcVPR+KVL0FMjeHIMk+IQi6XG9ugYVFauGOWUuyhXNyseZWs/a66JK0WCp/6bEP0nDZKnGQN8CQIBJwDBS2MAozuAPCrT/E+bd+i5VFhSqCl4Mlsmsy5ah0UTlllUSIUah2jjOyGdTKGYb5iBiAkBLxPR7msakaa1KRm/9Ah6KgQvWmdShs+JTKSVvB8aiwme+AdMK7PG82P/vBn+fiyOxUlMNP+uKNrT/L1Gx6X190SxJt6CAAgEkAAEL41Bk4JQRhp/VPmPr6Y0xXb0ykMp4g954qFR9X/icZFT7Gyi3yl2TrE/+ModvlhGsY4hHs0dhuEagZ4Zj5/mjtStoMs4y8OmheL8RnWmNx5XTUHXETgz6iKOiu3CbPlQzo8JXkGZiIvyNyNw8Daxp4zE71D9+4n9TuQ/Z5rnxcZ+09FlcqmwgM/FVcck2gfdejS3S5Nghew3bEIDs0EgqwhA8NIYzrggEEV3BIrDYjxP7DTUf3DFH231H/rY+nJHol5HjCkmDHKZmFSqd0zKPvFq4nN8HRM4mu2arBPg2ZOsvBZ0daxi8WWJU20jCTv9GP/JmDoTvMlxBTg4Ke+6FLyI5m+ExY6zYEmCFxM3dYwSJC+2TDyLFotrQqxjv1/l9qD+p44cCF5Sf1POEQ2AAAikiwAEL12kpTzFMnGJO1XeecQkTiVNicspOqsUP50/7Al/vJXLK6oh0d6kQIr2IHhKQvH3ylgItir5cizomrGJZWvibUwKRrxDqm1KnkA/KRPKJfXeq9vRWy7s3yv4q34zJOQreh5bwm9OXtAQj2GMYcJvPCb3qmWi25rqd6nM4ouqFH3izzp/B7QiF60/XOfQanHAdyCQzQQgeGmMrlIQEsSK/zBLqUr442+QUVNIQUK9ivGI72W9YvnY4b/4FXTys2pHItdR1KX5VtVXzWWy6EslZ+X76A7euaAnxCnOS7XjV8Q7vgi/SZANuxm8WBtJ4pDQAj4IAkqZUr6P/UZjgpYoeInLTYJUxja6TDx7JxdKiLdyeblA9DXhHw2xjt0LJvT6mNgOPoEACASPAAQvjTFLlILJnTH/kY5nXVTSlLiOorPKP+bK94pFEnY2CTsMxUKqt9qyoVpIflT1VX6dra8JsVCKFbOVUqxiostTEY+EOCngJe+8pZCrX6WgT25Timp03sakAXKnw0f9daIITcYmylEKWmIsdeRNmdlTbS/xVpXbl1w+6R+z2HYgs386fwfideq8SdiudZbB1yAAAsEjAMFLY8zUf0ijO4NcypWHZ7kv6j/4ChFQdjVhR6JeRywY24HriEe8LtVOQVdI4iso3mi2q5ifZW8T4zcpUxwLN4KuzTwWv4SdtxQ5PbCTfdJbIvp9rG7InTGmhLmJghc/HCp+A5NZs4Tfpc55r4mH0nUkMOF3r9oWEvql+KD6LSvmGL5N3K4NF8VMEACBABGA4KUxWEl/SMUfZNV9s5KkSWNnHFtPZg14CGLHothhR9tKrDv6nVISVDsteU6XlEIzNkl9NVsh2PPV8YvuzD0Q9ISduWQUjU38pHo91gk7dSuCp7E9ySbxakBA9VuJxUPcF09KuPwdxn8/OmKWEEvtZdS/1URxlN1UxTthW5DLKF41tzONC0MUq+AtCIBAcAlA8NIYO7UgRM/dUmR/uC8Jf/wnOxcVOHloTilpOstYvA9ePPMUq0b0Mb6Dmqxb851OXzWXzYIvk+IndpiJEp0cPw2hiq03KeixZRTc4/FWyIN6p5+8/ah2+BrMo/VOZpw0FsFXmgRUgqc4bDoZRw1ZisV68neWHOtoNlBx26PY78rsKlrt7dEothba1hw7vgQBEAgiAQheEKOGPmeEQNIOVesQnI70xoVNnEelJeixna88z2qPtfvgTYoDIzERvLg4yH8UEl+VopIRwL5uVC148pFviUIl4qwQdTEkNXeFtMeHLP9ZkNuHlfvgKTL2oh5RR2J/4vXH36i2M+XpIfFl8AYEQCAbCEDwsiGKGAMIgAAIgAAIgAAIKAhA8BQw8BYEQAAEQAAEQAAEsoEABC8boogxgAAIgAAIgAAIgICCAARPAQNvQQAEQAAEQAAEQCAbCEDwsiGKGAMIgAAIgAAIgAAIKAhA8BQw8BYEQAAEQAAEQAAEsoEABC8boogxgAAIgAAIgAAIgICCAARPAQNvQQAEQCDTBB49ekTPnj3LdDfQPgiAQMAJQPACHkB0HwRAILsI7Nixg7hgAgEQAAE3BCB4bugFZN22tjY6efIkjY6OBqTH6GZtbS2dP38eIEJIgOVu586dIRw5hgwCIOAlAQielzR9Wld1dTXl5OTQ0NCQT3uIbqkJPH/+nKZMmaL+Gp9DQACCF4IgY4ggkAYCELw0QM50ExC8TEfAfvt8DhYEzz63bFhj+/btyOBlQyAxBhDIMAEIXoYDkI7mpeANDw+nozm04QEBFrypU6d6UBOqCBoBFrxdu3YFrdvoLwiAgM8IQPB8FpBUdKeqqkocooXgpYJuauqE4KWGaxBqheAFIUroIwj4nwAEz/8xct1DCJ5rhGmvAIKXduS+aRCC55tQoCMgEGgCELxAh89a5yF41jj5aamnT5/iEK2fApLGvmzbtg2HaNPIG02BQLYSgOBla2QV45KCNzIyovgWb/1MAILn5+iktm8seLt3705tI6gdBEAg6wlA8LI+xEQfPnwQ5+BB8IITbBa8adOmBafD6KlnBCB4nqFERSAQagIQvBCEH4IXvCBD8IIXM696DMHziiTqAYFwE4DghSD+ELzgBfnJkyfI4AUvbJ70+K+//sIhWk9IohIQCDcBCF4I4i8FD48qC06wIXjBiZXXPWXB27Nnj9fVoj4QAIGQEYDghSDg79+/F+fgQfCCE2wWvOnTpwenw+ipZwQgeJ6hREUgEGoCELwQhB+CF7wgQ/CCFzOvegzB84ok6gGBcBOA4IUg/hC84AX58ePHyOAFL2ye9PjPP//EIVpPSKISEAg3AQheCOIvBW9sbCwEo82OIULwsiOOTkbBgrd3714nq2IdEAABEIgTgODFUWTvm8rKSnEOHgQvODFmwZsxY0ZwOoyeekYAgucZSlQEAqEmAMELQfgheMELMgQveDHzqscQPK9Ioh4QCDcBCF4I4g/BC16QHz16hAxe8MLmSY+3bt2KQ7SekEQlIBBuAhC8EMRfCt74+HgIRpsdQ4TgZUccnYyCBS8vL8/JqlgHBEAABOIEIHhxFNn7pqKiQpyDB8ELToxZ8L777rvgdBg99YwABM8zlKgIBEJNAIIXgvBD8IIXZAhe8GLmVY8heF6RRD0gEG4CELwQxB+CF7wgP3z4EBm84IXNkx5v2bIFh2g9IYlKQCDcBCB4IYi/FLyJiYkQjDY7hpjNgsfbIZ8uwIVv3cOP0BsZGRFleHiYhoaGRBkcHKSBgQHq7+8Xpa+vj3p7e0Xp6emh7u5uUbq6uqizs5M6OjpEaW9vp7a2NlFaW1spEomI0tLSQs3NzdTU1CRKY2MjNTQ0iFJfX091dXVUW1srSk1NDVVXV4tSVVVF/DxnLnxPST6nlX9TXN69e0dv374V5c2bN/T69Wt69eqVKOXl5VRWViZKaWkplZSUiFJcXEwvX76kFy9eiFJUVETPnz8X5dmzZ8SCd/HixezYkDEKEACBjBGA4GUMffoavnbtmjgHj2+9wY/A4lc+BMgSweXBgwd0//59Ue7du0d3796lO3fuiHL79m26deuWKDdv3qQbN26Icv36deJ6r169KsqVK1eosLBQlIKCAvr777/p8uXLoly6dIny8/NF4R3XhQsXRDl//jydO3eOzp49K8qZM2fo9OnTopw6dYpOnjxJJ06cEOX48eN07NgxUY4ePUpHjhwR5fDhw3To0CE6ePCgKAcOHKD9+/eLsm/fPpEJ4RPW+cax/AD33bt3i7Jr1y7auXOnKDt27KDt27fTtm3bROFHRfGtKrjw4TLe4XLZvHkzbdq0if744w9RNm7cSBs2bBBl/fr1tG7dOlq7dq0oa9asodWrV4uyatUq+v3330VZuXIlrVixgpYvXy7KsmXLaOnSpaIsWbKEfvvtN1q8eDHNnj2bPv30U1q0aJEov/76K/3yyy+iLFy4kBYsWEA///yzKPPnz6d58+aJMnfuXJozZw799NNPonA9P/74oyizZs2iH374QZR//OMf9P3339PMmTNF4fP98QRypQAAB6JJREFU+L57XPgZuNOmTaOpU6eKMmXKFPr2229F+eabb+jrr78W5auvvqIvv/ySvvjiC1E+//xz+uyzz0Thvn/yySf08ccfi/LRRx+JbTAnJycrX3l8cqw8bh4/F+bBXCQj5sXcuDBH5smF+TJnyTx9fx3QEgiAQLYSgOBla2QV42J5450N77R55y135LxTlzt43tnzTp8LSwDLgBQDlgQpDCwPLBFSKFgupGiwdLB8cGEZYSmRgpKbmyvEheWFJYZlRooNS44UHpYfliAuLEUsR1KUWJqkQLFMsVRJwWLZkuLFEsYyxoXFjCVNChvLmxQ5ljqWOyl6LH1SAFkGWQq5sCCyLEpxZImUQslyyZIphZPlU4ooS6kUVJZVllYpsCyzUmxZcll2pfiyBHP7zJnlWIoySzPLsxRplmop2CzbLN1cWMJZxqWYs6RLYWd5Z4mXQs9yL0WfpZ/lX/4j8PTpU+KMEhfOMHGmiQtnnjgDxZkoLpyZ4gwVF85YceaKC2eyOKPFmS0unOnijBcXzn5xJowzYlw4O8aZMi6cOeMMGhfOqHFmjTNsXDjjxpk3LpyJ44wcZ+a4cKaOM3ZcOIPHmTwunNnjDB9n+rhw5o8zgFw4I8iZQZkl5IwhZw65cCaRM4pcOMPImUbOOHLh7CNnIpEVV/yhwVsQAAFfEYDg+Soc6AwIgAAIgAAIgAAIuCcAwXPPEDWAAAiAAAiAAAiAgK8IQPB8FQ50BgRAAARAAARAAATcE4DguWeIGkAABEAABEAABEDAVwQgeL4KBzoDAiAAAiAAAiAAAu4JQPDcM0QNIAACIAACIAACIOArAhA8X4UDnQEBEAABEAABEAAB9wQgeO4ZogYQAAEQAAEQAAEQ8BUBCJ6vwoHOgAAIgAAIgAAIgIB7AhA89wxRAwiAAAiAAAiAAAj4igAEz1fhQGdAAARAAARAAARAwD0BCJ57hqgBBEAABEAABEAABHxFAILnq3CgMyAAAiAAAiAAAiDgngAEzz1D1AACIAACIAACIAACviIAwfNVONAZEAABEAABEAABEHBPAILnniFqAAEQAAEQAAEQAAFfEYDg+Soc6AwIgAAIgAAIgAAIuCcAwXPPEDWAAAiAAAiAAAiAgK8IQPB8FQ50BgRAAARAAARAAATcE4DguWeIGkAABEAABEAABEDAVwQgeL4KBzrjXwIRKlyUQzk52iWvJHU9L9uTQzmLCikSayJSkJvw2bzlMspTrE8leZSTk0dl5itiCRAAARAAgYASgOAFNHDodroJxARvj4YWCWHKoVRJnlrw7I7c7fp228PyIAACIAACmScAwct8DNCDQBAwEDwiEhKlJX8ejM2toLld34MhoAoQAAEQAIE0E4DgpRk4mgsqAQuCJw+DckZvUSEV8qFVPqQrvycicXhVcZhXK+snhEwuI+tR16H4zEQT1smR2UT1YeVcKuTjvFqHaCOFlCvbVPWZ65eHhcv48HB8uVh9QQ0p+g0CIAACWUwAgpfFwcXQvCRgJHiqebFDtjmqjF5U7hRSFJOq3AJ5dp0Utcnz4+JCqBA6KVvRtWJtK+ZTrF4pj0kZPLXgxfo72Y/kOuP9iI8peRkvaaMuEAABEAABdwQgeO74Ye3QEFBJnGLcUn6kUEUzZAqRE8uWUV5ODk1KVLSC6LoxoVOJWWyJ6MUdCoFLEDyxjrotRedkdk+xfmIGT2dcqr4k9FNWL8TQuG25KF5BAARAAATSSwCCl17eaC2wBGIiFD88qbyaViU56gwZj1lPhpSCprWehqApBU9TvFSMjTN42uJJlCh+yjbj1euNKb4A3oAACIAACGSKAAQvU+TRbsAIJAqPYee1RE18p5RC5fuoIOrJmlqulJ+V7/X6ZEXw4tnHeCWJ49VsB4IXp4U3IAACIOA3AhA8v0UE/fEpgUThMeykruCpMn3qSrTWy3AGTx5ShuCpg4XPIAACIOBvAhA8f8cHvfMNAZeCpzqnLT4spdRpLhNrV3EOXYJsWciiGWfwdMal6ktCm7LzFtqWi+IVBEAABEAgvQQgeOnljdYCS0BHhLTGo5Q2xXwhSTnKLF7y+W9CxhRPmYiuo3GrlbjwJQsgUWK9xoInzw9UXgCSXCcETxFIvAUBEACBABCA4AUgSOiiHwi4FzweRVzYYhdryEOgyhFGJW/yHnrifnpxoZu8J93kzVXk7VUmz+tLqDeWjeP714lz7bQEVLGMuM9d/HYo0Z5B8JQRwnsQAAEQ8D8BCJ7/Y4QeggAIgAAIgAAIgIAtAhA8W7iwMAiAAAiAAAiAAAj4nwAEz/8xQg9BAARAAARAAARAwBYBCJ4tXFgYBEAABEAABEAABPxPAILn/xihhyAAAiAAAiAAAiBgiwAEzxYuLAwCIAACIAACIAAC/icAwfN/jNBDEAABEAABEAABELBFAIJnCxcWBgEQAAEQAAEQAAH/E4Dg+T9G6CEIgAAIgAAIgAAI2CIAwbOFCwuDAAiAAAiAAAiAgP8JQPD8HyP0EARAAARAAARAAARsEYDg2cKFhUEABEAABEAABEDA/wQgeP6PEXoIAiAAAiAAAiAAArYIQPBs4cLCIAACIAACIAACIOB/Av8fc2PKsQ98AhgAAAAASUVORK5CYII=)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZWnR0tBaqMA"
      },
      "source": [
        "**Concept:** Bagging involves creating multiple models (typically of the same type), each trained on a **random subset of the training data**. The subsets are drawn with replacement, known as bootstrapping. The final prediction is made by averaging the predictions (for regression problems) or by taking a majority vote (for classification problems).\n",
        "\n",
        "Let's illustrate the concept of bagging with a simple and relatable example: predicting the quality of wine based on various features like acidity, sugar level, alcohol content, etc. Suppose you are a novice wine taster, and you want to develop a model to predict wine quality.\n",
        "\n",
        "**Individual Models Without Bagging**\n",
        "\n",
        "First, imagine training several individual models to predict wine quality. Each model is like a different wine taster, and they all have their unique tasting methods. One model might focus heavily on acidity, another on sugar levels, and yet another on alcohol content. Since each model (or taster) focuses on different aspects, their predictions for the same wine can vary widely. Some might be very accurate in certain instances and completely off in others, leading to a high variance in predictions.\n",
        "\n",
        "**Applying Bagging**\n",
        "\n",
        "Now, let's apply the bagging approach. Instead of relying on a single model's prediction, you create an ensemble of models, where each model is trained on a random subset of the training data (imagine each taster only tasting a random selection of the entire wine collection). Because the models are trained on different subsets of the data, they learn to predict wine quality based on different aspects and characteristics of the wine.\n",
        "\n",
        "**Combining Predictions**\n",
        "\n",
        "Once all models have made their predictions, you don't just pick the prediction of one model. Instead, you aggregate the predictions of all models, typically by taking the average (for regression problems like predicting a quality score) or the majority vote (for classification problems, e.g., classifying wine into categories like high, medium, or low quality).\n",
        "\n",
        "**The Result**\n",
        "\n",
        "By aggregating the predictions, the ensemble's final prediction is less sensitive to the peculiarities of any single model. If one model predicts the quality to be much higher than it should be based on its unique tasting criteria, and another predicts it much lower, these extreme predictions can cancel each other out in the averaging process, leading to a more accurate and stable final prediction.\n",
        "\n",
        "**Why It Works**\n",
        "\n",
        "This method works well because it effectively reduces the variance without significantly increasing the bias. No single model's quirks dominate the final prediction, making the ensemble's prediction more reliable than any individual model's guess. It's like consulting a group of wine tasters instead of relying on just one—they might each make some mistakes, but their collective judgment is likely to be more accurate on average."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPUqyRgca58o"
      },
      "source": [
        "**Example:** *Random Forest* is a classic example of bagging where multiple decision trees are trained on different subsets of the training data. For a regression problem, if five trees predict the values of 10, 12, 11, 9, and 10 for a particular sample, the final prediction would be the average: $ (10 + 12 + 11 + 9 + 10) / 5 = 10.4 $. For a classification problem, if the prediction from five trees for a sample is [cat, cat, dog, cat, dog], the final prediction would be 'cat' as it is the majority."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "L7aIYgFTZ-ny"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R^2 Score: 0.7844762342339637\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load the California housing dataset\n",
        "data = fetch_california_housing()\n",
        "feature_names = data.feature_names\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the base estimators\n",
        "estimator_1 = DecisionTreeRegressor()\n",
        "estimator_2 = LinearRegression()\n",
        "\n",
        "# Initialize the Bagging regressor with 10 estimators\n",
        "bagging_regressor = BaggingRegressor(estimator=estimator_1, n_estimators=10, random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = bagging_regressor.predict(X_test)\n",
        "\n",
        "# Evaluate the model using R^2 score\n",
        "r2 = r2_score(y_test, predictions)\n",
        "print(\"R^2 Score:\", r2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1XW2DWbmgl0D"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tree 1 feature importances:\n",
            "MedInc: 0.5371171195896982\n",
            "HouseAge: 0.060559266808299504\n",
            "AveRooms: 0.05291099123902993\n",
            "AveBedrms: 0.026699855754815104\n",
            "Population: 0.03939129930105988\n",
            "AveOccup: 0.13417093185361062\n",
            "Latitude: 0.07074932373682256\n",
            "Longitude: 0.0784012117166643\n",
            "---------------------------------\n",
            "Tree 2 feature importances:\n",
            "MedInc: 0.5046072955577671\n",
            "HouseAge: 0.0511857522928054\n",
            "AveRooms: 0.03615316664506402\n",
            "AveBedrms: 0.0278091255121323\n",
            "Population: 0.027705122416302613\n",
            "AveOccup: 0.14180752409843142\n",
            "Latitude: 0.10588456095751816\n",
            "Longitude: 0.10484745251997912\n",
            "---------------------------------\n",
            "Tree 3 feature importances:\n",
            "MedInc: 0.5127410934107334\n",
            "HouseAge: 0.053160563174034774\n",
            "AveRooms: 0.0338921117848507\n",
            "AveBedrms: 0.027419112479241625\n",
            "Population: 0.03092681126615477\n",
            "AveOccup: 0.1473139381777373\n",
            "Latitude: 0.09751599024257661\n",
            "Longitude: 0.09703037946467089\n",
            "---------------------------------\n",
            "Tree 4 feature importances:\n",
            "MedInc: 0.5419597585053457\n",
            "HouseAge: 0.0571112492964541\n",
            "AveRooms: 0.05469966215131055\n",
            "AveBedrms: 0.03299288255193581\n",
            "Population: 0.025410093908372497\n",
            "AveOccup: 0.13029776304759996\n",
            "Latitude: 0.07768350653071417\n",
            "Longitude: 0.0798450840082671\n",
            "---------------------------------\n",
            "Tree 5 feature importances:\n",
            "MedInc: 0.5234678506655452\n",
            "HouseAge: 0.05392400702211487\n",
            "AveRooms: 0.05296375764262388\n",
            "AveBedrms: 0.031041297435566443\n",
            "Population: 0.023359655824125306\n",
            "AveOccup: 0.12987002148025198\n",
            "Latitude: 0.09346477966945382\n",
            "Longitude: 0.09190863026031854\n",
            "---------------------------------\n",
            "Tree 6 feature importances:\n",
            "MedInc: 0.5267992610819863\n",
            "HouseAge: 0.05059221089673813\n",
            "AveRooms: 0.050160181473253816\n",
            "AveBedrms: 0.03201087873363498\n",
            "Population: 0.029121629848823098\n",
            "AveOccup: 0.13096802256465964\n",
            "Latitude: 0.09110410572548688\n",
            "Longitude: 0.08924370967541723\n",
            "---------------------------------\n",
            "Tree 7 feature importances:\n",
            "MedInc: 0.5277104222602063\n",
            "HouseAge: 0.05887092589654724\n",
            "AveRooms: 0.05625410867675141\n",
            "AveBedrms: 0.02996586530801487\n",
            "Population: 0.03268539852057031\n",
            "AveOccup: 0.13669547851297606\n",
            "Latitude: 0.07745901342845793\n",
            "Longitude: 0.08035878739647598\n",
            "---------------------------------\n",
            "Tree 8 feature importances:\n",
            "MedInc: 0.5250958788423334\n",
            "HouseAge: 0.056395433978936864\n",
            "AveRooms: 0.05123808332441416\n",
            "AveBedrms: 0.02695041164355372\n",
            "Population: 0.031573410460645455\n",
            "AveOccup: 0.1445281623615447\n",
            "Latitude: 0.08238213722204844\n",
            "Longitude: 0.0818364821665231\n",
            "---------------------------------\n",
            "Tree 9 feature importances:\n",
            "MedInc: 0.5356462589280813\n",
            "HouseAge: 0.06345383784800206\n",
            "AveRooms: 0.048001838891489755\n",
            "AveBedrms: 0.03177609479973673\n",
            "Population: 0.03414826683375841\n",
            "AveOccup: 0.13079241703566377\n",
            "Latitude: 0.07995682588833833\n",
            "Longitude: 0.07622445977492977\n",
            "---------------------------------\n",
            "Tree 10 feature importances:\n",
            "MedInc: 0.5454378558625149\n",
            "HouseAge: 0.05465409606759686\n",
            "AveRooms: 0.04812253947851265\n",
            "AveBedrms: 0.02601709931265048\n",
            "Population: 0.0336265130546061\n",
            "AveOccup: 0.13565547879950887\n",
            "Latitude: 0.07680245746306234\n",
            "Longitude: 0.07968395996154773\n",
            "---------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Extract feature importances for each tree\n",
        "for i, tree in enumerate(bagging_regressor.estimators_):\n",
        "    print(f\"Tree {i+1} feature importances:\")\n",
        "    for feature_name, importance in zip(feature_names, tree.feature_importances_):\n",
        "        print(f\"{feature_name}: {importance}\")\n",
        "    print(\"---------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2cta4-QjKJ3"
      },
      "source": [
        "#### [Randrom Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TzHfaZy0jMYZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R^2 Score: 0.7831038080291017\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load the California housing dataset\n",
        "data = fetch_california_housing()\n",
        "feature_names = data.feature_names\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the RandomForestRegressor\n",
        "random_forest_regressor = RandomForestRegressor(n_estimators=10, random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "random_forest_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = random_forest_regressor.predict(X_test)\n",
        "\n",
        "# Evaluate the model using R^2 score\n",
        "r2 = r2_score(y_test, predictions)\n",
        "print(\"R^2 Score:\", r2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WEbtqrlSjJpC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tree 1 feature importances:\n",
            "MedInc: 0.5364110252199703\n",
            "HouseAge: 0.061136179837297\n",
            "AveRooms: 0.05462270271250828\n",
            "AveBedrms: 0.028007715572215597\n",
            "Population: 0.038459826957715625\n",
            "AveOccup: 0.1336949556167921\n",
            "Latitude: 0.07012355251208194\n",
            "Longitude: 0.0775440415714191\n",
            "---------------------------------\n",
            "Tree 2 feature importances:\n",
            "MedInc: 0.5050215843894205\n",
            "HouseAge: 0.05082413149470552\n",
            "AveRooms: 0.036140932433465826\n",
            "AveBedrms: 0.028220309938613086\n",
            "Population: 0.025531322772685515\n",
            "AveOccup: 0.1430348915220854\n",
            "Latitude: 0.10740369793668152\n",
            "Longitude: 0.10382312951234256\n",
            "---------------------------------\n",
            "Tree 3 feature importances:\n",
            "MedInc: 0.5130268376859589\n",
            "HouseAge: 0.05467030265920032\n",
            "AveRooms: 0.03339948513315309\n",
            "AveBedrms: 0.027521557368732193\n",
            "Population: 0.02959824142693077\n",
            "AveOccup: 0.14702819130503902\n",
            "Latitude: 0.09794503538167539\n",
            "Longitude: 0.09681034903931032\n",
            "---------------------------------\n",
            "Tree 4 feature importances:\n",
            "MedInc: 0.5409285006537908\n",
            "HouseAge: 0.05689548232524607\n",
            "AveRooms: 0.055226635748400976\n",
            "AveBedrms: 0.03327282056062452\n",
            "Population: 0.025434121607117714\n",
            "AveOccup: 0.13035711204410608\n",
            "Latitude: 0.07821131484906074\n",
            "Longitude: 0.07967401221165293\n",
            "---------------------------------\n",
            "Tree 5 feature importances:\n",
            "MedInc: 0.52279783925685\n",
            "HouseAge: 0.053577646909374534\n",
            "AveRooms: 0.053328292764143144\n",
            "AveBedrms: 0.031061083179573488\n",
            "Population: 0.023500133281000046\n",
            "AveOccup: 0.13148112778189533\n",
            "Latitude: 0.0911054291072628\n",
            "Longitude: 0.09314844771990062\n",
            "---------------------------------\n",
            "Tree 6 feature importances:\n",
            "MedInc: 0.5271333299716751\n",
            "HouseAge: 0.049859581509271486\n",
            "AveRooms: 0.05256728586753125\n",
            "AveBedrms: 0.03382844761987374\n",
            "Population: 0.028181055363575326\n",
            "AveOccup: 0.13020702308979948\n",
            "Latitude: 0.08991609511141886\n",
            "Longitude: 0.08830718146685472\n",
            "---------------------------------\n",
            "Tree 7 feature importances:\n",
            "MedInc: 0.5286835788028891\n",
            "HouseAge: 0.05901479735767306\n",
            "AveRooms: 0.05656687383304404\n",
            "AveBedrms: 0.029230939955816746\n",
            "Population: 0.031246292833768986\n",
            "AveOccup: 0.1365229828990768\n",
            "Latitude: 0.07746520315559384\n",
            "Longitude: 0.08126933116213744\n",
            "---------------------------------\n",
            "Tree 8 feature importances:\n",
            "MedInc: 0.5238841927568243\n",
            "HouseAge: 0.05676769477555715\n",
            "AveRooms: 0.0528868069018393\n",
            "AveBedrms: 0.026982751084780092\n",
            "Population: 0.03229863856086119\n",
            "AveOccup: 0.14271008484289624\n",
            "Latitude: 0.08241346581650436\n",
            "Longitude: 0.08205636526073731\n",
            "---------------------------------\n",
            "Tree 9 feature importances:\n",
            "MedInc: 0.5361063106936903\n",
            "HouseAge: 0.062302573108030126\n",
            "AveRooms: 0.047732771949401884\n",
            "AveBedrms: 0.03110206124371674\n",
            "Population: 0.035825561936702686\n",
            "AveOccup: 0.1314091514210963\n",
            "Latitude: 0.07912289161815167\n",
            "Longitude: 0.0763986780292102\n",
            "---------------------------------\n",
            "Tree 10 feature importances:\n",
            "MedInc: 0.5462232696984177\n",
            "HouseAge: 0.053742694637022206\n",
            "AveRooms: 0.04820816861119263\n",
            "AveBedrms: 0.026166159317561857\n",
            "Population: 0.034362262924148804\n",
            "AveOccup: 0.13600876836646064\n",
            "Latitude: 0.07639137003100953\n",
            "Longitude: 0.07889730641418664\n",
            "---------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Extract feature importances for each tree\n",
        "for i, tree in enumerate(random_forest_regressor.estimators_):\n",
        "    print(f\"Tree {i+1} feature importances:\")\n",
        "    for feature_name, importance in zip(feature_names, tree.feature_importances_):\n",
        "        print(f\"{feature_name}: {importance}\")\n",
        "    print(\"---------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGQvlR55Y7sy"
      },
      "source": [
        "### 2. Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPt_P-NOX8-e"
      },
      "source": [
        "Boosting involves sequentially training models, where each new model attempts to correct errors made by the previous models. The models are weighted based on their accuracy, and predictions are made by weighting the contribution of each model. Boosting can reduce bias and variance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjor9HH9hKq6"
      },
      "source": [
        "#### AdaBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amdL1oxRhQIG"
      },
      "source": [
        "Boosting is a powerful ensemble learning technique where multiple weak learners are combined to create a strong learner. AdaBoost, short for Adaptive Boosting, is a popular algorithm that illustrates the concept of boosting. Here's how it works:\n",
        "\n",
        "Imagine you're preparing for a difficult exam, and you decide to seek help from multiple tutors. Each tutor has their strengths and weaknesses, but you combine their expertise to improve your understanding overall.\n",
        "\n",
        "1. **Initial Model (Tutor)**: You start by studying with the first tutor, who helps you learn the material. However, they may not cover all topics comprehensively, leading to some gaps in your understanding.\n",
        "\n",
        "2. **Weights on Incorrectly Predicted Instances**: After the first study session, you identify the topics where you struggled the most. You realize that these topics are crucial for the exam, so you decide to focus more on them. Similarly, AdaBoost assigns higher weights to instances (data points) that were incorrectly predicted by the initial model, essentially highlighting the \"difficult\" cases.\n",
        "\n",
        "3. **Sequential Improvement**: For the next study session, you bring in a new tutor who specializes in the topics you found challenging. This tutor helps you understand those specific areas better, improving your overall grasp of the material. Similarly, AdaBoost trains a new model, giving more emphasis to the instances that were misclassified by the previous model. This new model focuses on correcting the errors made by its predecessor.\n",
        "\n",
        "4. **Combining Predictions**: You continue this process, bringing in additional tutors for each subsequent study session, with each tutor addressing the topics where you need the most help. Eventually, you combine the insights from all tutors to create a comprehensive understanding of the material. In AdaBoost, predictions from all models are combined through a weighted voting mechanism, where each model's contribution is proportional to its accuracy.\n",
        "\n",
        "5. **Final Prediction**: By the end of your study sessions, you have gained a deep understanding of the material, covering all important topics extensively. Similarly, AdaBoost generates the final prediction by combining the predictions of all models, resulting in a strong and accurate overall prediction.\n",
        "\n",
        "In summary, AdaBoost iteratively improves the model's performance by focusing on the instances where it performs poorly, effectively boosting its ability to make accurate predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "H8XJkuuFhn_6"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "AdaBoostRegressor.__init__() got an unexpected keyword argument 'base_estimator'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Initialize AdaBoost Regressor with DecisionTreeRegressor as the base estimator\u001b[39;00m\n\u001b[0;32m     15\u001b[0m base_estimator \u001b[38;5;241m=\u001b[39m DecisionTreeRegressor(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m adaboost_reg \u001b[38;5;241m=\u001b[39m \u001b[43mAdaBoostRegressor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Fit the model to the training data\u001b[39;00m\n\u001b[0;32m     19\u001b[0m adaboost_reg\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
            "\u001b[1;31mTypeError\u001b[0m: AdaBoostRegressor.__init__() got an unexpected keyword argument 'base_estimator'"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load California housing dataset\n",
        "california_housing = fetch_california_housing()\n",
        "X, y = california_housing.data, california_housing.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize AdaBoost Regressor with DecisionTreeRegressor as the base estimator\n",
        "base_estimator = DecisionTreeRegressor(max_depth=4)\n",
        "adaboost_reg = AdaBoostRegressor(base_estimator=base_estimator, n_estimators=50, learning_rate=1.0)\n",
        "\n",
        "# Fit the model to the training data\n",
        "adaboost_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing data\n",
        "y_pred = adaboost_reg.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"R^2 Score:\", r2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUMacvwiT5JR"
      },
      "source": [
        "#### [Gradient Boost](https://www.youtube.com/watch?v=Nol1hVtLOSg&list=PLc-A_ClQmXKDlVbr9V3VhDZC5jJsPBN3Q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHQJCvTzT77W"
      },
      "source": [
        "Gradient Boosting is a powerful machine learning technique used for regression and classification tasks. It sequentially builds an ensemble of models, each aimed at correcting the errors of its predecessors. Beginning with a simple initial model, such as a decision tree, subsequent models focus on predicting the residuals (the differences between actual and predicted values) of the previous ensemble. By minimizing a loss function using gradient descent during training, each new model targets the negative gradients of the loss function with respect to the ensemble's predictions. Finally, predictions from all models are combined, with each model's contribution weighted based on its performance, resulting in a final prediction that effectively reduces overall prediction error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dPmDYb3_7j5"
      },
      "source": [
        "#### XGBoost (eXtreme Gradient Boosting)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzU4BZmo_8W0"
      },
      "source": [
        "XGBoost (eXtreme Gradient Boosting) is an advanced and efficient implementation of gradient boosting, a machine learning algorithm for regression, classification, ranking, and other predictive tasks. Developed by Tianqi Chen, it has become popular in machine learning competitions for its performance and speed. XGBoost is designed to be highly efficient, flexible, and portable. It can handle various types of data and is capable of performing on large datasets with ease.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PHD0ZiIiCoF"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load the California housing dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the XGBoost regressor\n",
        "xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "xgb_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = xgb_reg.predict(X_test)\n",
        "\n",
        "# Evaluate the model using R^2 score\n",
        "r2 = r2_score(y_test, predictions)\n",
        "print(\"R^2 Score:\", r2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNn90yl4Y9Ti"
      },
      "source": [
        "### 3. Stacking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMYryWjeX-4Q"
      },
      "source": [
        "**Concept:** Stacking involves training multiple models (potentially of different types) and then training a meta-model on their predictions. The base models are trained on the full training set, and their predictions are used as input features for the meta-model to make the final prediction.\n",
        "\n",
        "**Example:** Suppose we have three different models: a decision tree, a support vector machine, and a neural network, predicting house prices. Each model predicts prices on the training data, and then a meta-model (e.g., linear regression) is trained on these predictions to make the final price prediction. For a particular house, if the three models predict prices of 200,000; 210,000; and 205,000, these values serve as input features for the meta-model to make the final decision.\n",
        "\n",
        "In summary, bagging reduces variance and is useful when the model is complex and overfitting. Boosting can adjust for bias and variance but is sensitive to noisy data and outliers. Stacking attempts to harness the strengths of multiple models to improve overall performance, which can be particularly powerful if the base models make errors in different parts of the input space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Rjso8IR_ZtV"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load the California housing dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the base estimators\n",
        "estimators = [\n",
        "    ('dt', DecisionTreeRegressor(random_state=42)),\n",
        "    ('linear', LinearRegression()),\n",
        "    ('svr', SVR(kernel='linear'))\n",
        "]\n",
        "\n",
        "# Initialize the stacking regressor\n",
        "stacked_reg = StackingRegressor(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LinearRegression()  # You can use any final estimator here\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "stacked_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = stacked_reg.predict(X_test)\n",
        "\n",
        "# Evaluate the model using R^2 score\n",
        "r2 = r2_score(y_test, predictions)\n",
        "print(\"R^2 Score:\", r2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8q6zGOjzhBuI"
      },
      "source": [
        "## Additional Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB7rMxYQ_wXx"
      },
      "source": [
        "### AdaBoost Error Calculation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGF1UQHP-oYA"
      },
      "source": [
        "##### AdaBoost Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OtFXX3a-REo"
      },
      "source": [
        "For a regression example with boosting, let's consider a simplified case using Gradient Boosting, another popular boosting method that focuses on minimizing the residual errors of the models. Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. This method can be used for both classification and regression tasks. Here, we'll focus on a regression task with a very simple dataset.\n",
        "\n",
        "Imagine we have a dataset with just one feature (x) and a continuous target variable (y) to predict:\n",
        "\n",
        "| x  | y  |\n",
        "|----|----|\n",
        "| 1  | 2  |\n",
        "| 2  | 3  |\n",
        "| 3  | 5  |\n",
        "| 4  | 7  |\n",
        "| 5  | 8  |\n",
        "\n",
        "We aim to predict \\(y\\) given \\(x\\) using Gradient Boosting for regression. The steps we'll follow are:\n",
        "\n",
        "1. **Initialize with a base model**: Often, this is just the mean of the target variable, but it could be a different simple model.\n",
        "2. **Compute residuals**: Calculate the difference between the predictions and the actual target values.\n",
        "3. **Fit a model to the residuals**: Train a model on the original features to predict these residuals.\n",
        "4. **Predict the target using all models**: Add the predictions of the new model to the predictions of the previous step.\n",
        "5. **Repeat steps 2-4**: Keep fitting new models to the residuals left by the current ensemble of models.\n",
        "\n",
        "For simplicity, let's simulate this process manually through two iterations, using very basic models (e.g., decision trees with depth = 1, also known as decision stumps) for step 3, and calculate predictions.\n",
        "\n",
        "1. **Initialize with the mean of y**: \\(mean(y) = 5\\)\n",
        "2. **Compute residuals after the first model** (which is just the mean):\n",
        "   - Residuals: \\([-3, -2, 0, 2, 3]\\)\n",
        "3. **Fit a model to these residuals**. For simplicity, let's say this model predicts +2 for \\(x > 3\\) and -2.5 for \\(x \\leq 3\\).\n",
        "4. **Update predictions** by adding the predictions of this new model to the mean.\n",
        "\n",
        "Let's calculate the updated predictions after adding this model and then proceed to a second iteration to illustrate how Gradient Boosting iteratively reduces the residual error.\n",
        "\n",
        "Here's how the process unfolded:\n",
        "\n",
        "- **Initial predictions** were all 5, the mean of \\(y\\).\n",
        "- After **computing the residuals**, we observed differences between the actual values of \\(y\\) and our predictions. These residuals were \\([-3, -2, 0, 2, 3]\\).\n",
        "- Our **simplified first model** was applied, resulting in adjusted predictions of \\([2.5, 2.5, 2.5, 7, 7]\\). This model effectively added corrections based on the feature \\(x\\).\n",
        "- After adding these corrections, the **new residuals** became \\([-0.5, 0.5, 2.5, 0, 1]\\).\n",
        "- For our **second iteration**, we assumed a model that made further adjustments, resulting in **updated predictions** of \\([3, 3, 3, 6, 6]\\).\n",
        "\n",
        "This example simplifies the actual models and calculations that would occur in gradient boosting but illustrates the iterative process of fitting models to residuals to improve predictions. In each iteration, the model tries to correct the mistakes of the ensemble of previous models, progressively improving the accuracy of the predictions. In practice, gradient boosting would use decision trees as weak learners and optimize their parameters to minimize the loss function, which in the case of regression, is often mean squared error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6pCc1N5-2P5"
      },
      "source": [
        "##### AdaBoost Classfication"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEgcmu6F9Q6J"
      },
      "source": [
        "Let's assume we have a dataset with five data points and a binary classification task. The features are just single values for simplicity, and the goal is to classify each data point as either +1 or -1.\n",
        "\n",
        "| Data Point | Feature (x) | Label (y) |\n",
        "|------------|-------------|-----------|\n",
        "| 1          | 1           | +1        |\n",
        "| 2          | 2           | +1        |\n",
        "| 3          | 3           | -1        |\n",
        "| 4          | 4           | -1        |\n",
        "| 5          | 5           | +1        |\n",
        "\n",
        "We'll follow these steps:\n",
        "\n",
        "1. **Initialize weights**: Each data point is given an equal initial weight. Since we have 5 data points, each weight is 1/5 = 0.2.\n",
        "2. **Train a weak learner**: Fit a weak model (like a decision stump) to the data. The goal is to minimize weighted error.\n",
        "3. **Calculate the error**: Error is calculated as the sum of weights where the prediction is wrong.\n",
        "4. **Compute the model's weight (alpha)**: This is based on the error; a lower error means a higher weight for the model.\n",
        "5. **Update data point weights**: Increase weights for wrongly predicted points, so the next model pays more attention to them.\n",
        "6. **Repeat**: Go back to step 2, using the updated weights.\n",
        "\n",
        "Let's simulate this process for the first iteration:\n",
        "\n",
        "1. Initial weights: 0.2 for each data point.\n",
        "2. Train a weak learner (decision stump). Let's say it splits at `x < 3.5` to predict +1 for `x < 3.5` and -1 otherwise. This model makes a mistake on data points 4 and 5.\n",
        "3. Calculate error: $error = 0.2 (for\\ 4) + 0.2 (for\\ 5) = 0.4$.\n",
        "4. Compute the model's weight ($\\alpha$): $\\alpha = \\frac{1}{2} \\log \\left(\\frac{1-error}{error}\\right) = \\frac{1}{2} \\log \\left(\\frac{0.6}{0.4}\\right)$.\n",
        "5. Update weights: Increase for wrongly predicted (4, 5) and decrease for correctly predicted. The formula for updating is $weight_{new} = weight_{old} \\cdot \\exp(\\alpha)$ for incorrectly classified and $weight_{new} = weight_{old} \\cdot \\exp(-\\alpha)$ for correctly classified.\n",
        "\n",
        "Let's calculate the exact values for $\\alpha$ and the updated weights.\n",
        "\n",
        "After the first iteration of boosting with AdaBoost, the model's weight ($\\alpha$) is approximately 0.203. This weight reflects the importance of this first model in the final prediction.\n",
        "\n",
        "The updated weights for each data point are as follows:\n",
        "\n",
        "- Data Point 1: 0.163\n",
        "- Data Point 2: 0.163\n",
        "- Data Point 3: 0.163\n",
        "- Data Point 4: 0.245 (incorrectly predicted, hence the weight increased)\n",
        "- Data Point 5: 0.245 (incorrectly predicted, hence the weight increased)\n",
        "\n",
        "These updated weights will be used in the next iteration to help the new model focus more on the incorrectly predicted instances (4 and 5 in this case). This process repeats, with each new model focusing more on the mistakes of the previous ones until a stopping condition is met, such as a maximum number of models or an acceptable error rate."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "DemTFshpYLwB",
        "RBpr79LsYRjQ",
        "qjor9HH9hKq6",
        "OUMacvwiT5JR",
        "8dPmDYb3_7j5",
        "sNn90yl4Y9Ti",
        "8q6zGOjzhBuI",
        "RB7rMxYQ_wXx",
        "AGF1UQHP-oYA",
        "Y6pCc1N5-2P5"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
