{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jwCXlnQcTo2t",
        "4UNjzcMzUE51",
        "vF1juGz6AKxv",
        "RRW-fhqLMqW3",
        "zO3ZlCarPpDy",
        "Z54wAqCxMZmt",
        "FLWa322hZLxh"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Text Preprocessing"
      ],
      "metadata": {
        "id": "jwCXlnQcTo2t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regular Expression"
      ],
      "metadata": {
        "id": "4UNjzcMzUE51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Python, regular expressions (regex) are sequences of characters that define a search pattern, used mainly for pattern matching within strings. They allow you to search, match, and manipulate text based on certain patterns rather than exact matches.\n",
        "\n",
        "Here's a brief overview of how regular expressions work in Python:\n",
        "\n",
        "1. **Pattern Definition**: Regular expressions are defined as strings in Python, using functions like `re.search()`, `re.match()`, `re.findall()`, `re.sub()`, `re.split()`, etc.\n",
        "\n",
        "2. **Metacharacters**: Regular expressions consist of both literal characters and metacharacters. Metacharacters are characters with a special meaning. For example:\n",
        "\n",
        "    1. **`.` (Dot)**:\n",
        "        - Matches any single character except newline (`\\n`).\n",
        "        - For example, `a.c` would match 'abc', 'adc', 'aec', etc., but not 'ac' or 'abbc'.\n",
        "\n",
        "    2. **`^` (Caret)**:\n",
        "        - Matches the start of a string.\n",
        "        - For example, `^hello` would match 'hello' only if it appears at the beginning of a string.\n",
        "\n",
        "    3. **`$` (Dollar)**:\n",
        "        - Matches the end of a string.\n",
        "        - For example, `world$` would match 'world' only if it appears at the end of a string.\n",
        "\n",
        "    4. **`[]` (Square Brackets)**:\n",
        "        - Defines a character class, allowing you to specify a set of characters that you want to match.\n",
        "        - For example, `[aeiou]` matches any vowel, `[0-9]` matches any digit, and `[abc]` matches 'a', 'b', or 'c'.\n",
        "\n",
        "    5. **`|` (Pipe)**:\n",
        "        - Represents alternation, allowing you to specify alternatives.\n",
        "        - For example, `cat|dog` matches either 'cat' or 'dog'.\n",
        "\n",
        "    6. **`*` (Asterisk)**:\n",
        "        - Matches zero or more occurrences of the preceding character or group.\n",
        "        - For example, `ba*t` would match 'bt', 'bat', 'baat', 'baaat', and so on.\n",
        "\n",
        "    7. **`+` (Plus)**:\n",
        "        - Matches one or more occurrences of the preceding character or group.\n",
        "        - For example, `ba+t` would match 'bat', 'baat', 'baaat', and so on, but not 'bt'.\n",
        "\n",
        "    8. **`?` (Question Mark)**:\n",
        "        - Matches zero or one occurrence of the preceding character or group, indicating it is optional.\n",
        "        - For example, `colou?r` would match both 'color' and 'colour'.\n",
        "\n",
        "    9. **`{}` (Curly Brackets)**:\n",
        "        - Specifies the exact number of occurrences or a range of occurrences.\n",
        "        - For example, `a{3}` matches 'aaa', and `a{1,3}` matches 'a', 'aa', or 'aaa'.\n",
        "\n",
        "    10. **`\\` (Backslash)**:\n",
        "        - Used as an escape character to treat metacharacters as literal characters.\n",
        "        - For example, `\\.` matches a literal dot.\n",
        "\n",
        "    11. **`()` (Round Brackets)**:\n",
        "        - Creates a capturing group, allowing you to capture and extract substrings or apply quantifiers to a group of characters.\n",
        "        - For example, `(abc)+` matches 'abc', 'abcabc', 'abcabcabc', and so on.\n",
        "    12. **`\\b` (Word Boundary)**\n",
        "      - This is a zero-width assertion that matches the position between a word character (like letters, digits, and underscores) and a non-word character. It ensures that the pattern matches complete words or strings that stand alone.\n",
        "\n",
        "3. **Functions**: Python provides several functions for working with regular expressions:\n",
        "   - `re.search()`: Searches for the pattern within the string and returns the first match.\n",
        "   - `re.match()`: Checks for a match only at the beginning of the string.\n",
        "   - `re.findall()`: Returns a list of all non-overlapping matches.\n",
        "   - `re.sub()`: Replaces occurrences of the pattern with a replacement string.\n",
        "   - `re.split()`: Splits the string by occurrences of the pattern.\n",
        "\n",
        "4. **Anchors**: Anchors like `^` and `$` are used to match the start and end of a string, respectively.\n",
        "\n",
        "5. **Character Classes**: `[ ]` is used to specify a character class. For example, `[aeiou]` matches any vowel.\n",
        "\n",
        "6. **Modifiers**: Modifiers like `*`, `+`, and `?` are used to specify the number of occurrences of the preceding character or group.\n",
        "\n",
        "7. **Escape Character**: `\\` is used as an escape character to treat metacharacters as literal characters. For example, `\\.` matches a literal dot.\n",
        "\n",
        "8. **Alternation**: `|` is used to specify alternatives. For example, `cat|dog` matches either 'cat' or 'dog'."
      ],
      "metadata": {
        "id": "4t7iH8wBYOpw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `finadall()`"
      ],
      "metadata": {
        "id": "bjWvDXJ6_t75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# Regular expression pattern for matching email addresses\n",
        "pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "text = \"Hello, you can reach out to mam@gmail.com or contact support@domain.com for more details.\"\n",
        "# Use re.findall to extract all email addresses from the text\n",
        "emails = re.findall(pattern, text)\n",
        "print(emails)"
      ],
      "metadata": {
        "id": "jVeDnInK97qY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42ef3d0d-1e6e-4039-fe0a-09468de57f8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['mam@gmail.com', 'support@domain.com']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pattern Breakdown\n",
        "\n",
        "1. **`[a-zA-Z0-9._%+-]+`**\n",
        "   - **`[a-zA-Z0-9._%+-]`**: This is a character class that matches any single alphanumeric character (both uppercase and lowercase), period (`.`), underscore (`_`), percent (`%`), plus (`+`), or hyphen (`-`).\n",
        "   - **`+`**: This quantifier matches one or more of the preceding character class, allowing for email local parts that can be of varying lengths.\n",
        "\n",
        "2. **`@`**\n",
        "   - This matches the literal `@` symbol in the email address.\n",
        "\n",
        "3. **`[a-zA-Z0-9.-]+`**\n",
        "   - **`[a-zA-Z0-9.-]`**: This character class matches any single alphanumeric character (both uppercase and lowercase), period (`.`), or hyphen (`-`) in the domain part of the email.\n",
        "   - **`+`**: This quantifier matches one or more of the preceding character class, allowing for domain names of varying lengths.\n",
        "\n",
        "4. **`\\.`**\n",
        "   - This matches the literal period (`.`) before the top-level domain. The backslash `\\` is used to escape the period since a plain period matches any character in regular expressions.\n",
        "\n",
        "5. **`[a-zA-Z]{2,}`**\n",
        "   - **`[a-zA-Z]`**: This character class matches any single uppercase or lowercase letter.\n",
        "   - **`{2,}`**: This quantifier specifies that the preceding character class (letters) must occur at least 2 times, which is typical for top-level domains (e.g., `.com`, `.org`)."
      ],
      "metadata": {
        "id": "DP_C6FrA_U8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `sub`"
      ],
      "metadata": {
        "id": "vF1juGz6AKxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# Regular expression pattern for matching email addresses\n",
        "pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "# Text containing email addresses\n",
        "text = \"Please contact us at support@example.com or sales@example.org.\"\n",
        "# Use re.sub to replace email addresses with '[REDACTED]'\n",
        "redacted_text = re.sub(pattern, '[REDACTED]', text)\n",
        "print(redacted_text)"
      ],
      "metadata": {
        "id": "lOmTuu0bAOgq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00b71591-4e10-49ba-b9d8-5d8c5bfddf44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please contact us at [REDACTED] or [REDACTED].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization"
      ],
      "metadata": {
        "id": "RRW-fhqLMqW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization is the process of dividing text into individual units or tokens. The choice of tokens depends on the granularity needed for the analysis. Common types of tokens include:\n",
        "\n",
        "**Sentence Tokenization**:\n",
        "Divides text into sentences. Useful for tasks where understanding the structure of the text is important.\n",
        "```python\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "```\n",
        "\n",
        "```python\n",
        "from nltk.tokenize import sent_tokenize\n",
        "text = \"Natural language processing is amazing. It allows computers to understand human language.\"\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)\n",
        "# Output: ['Natural language processing is amazing.', 'It allows computers to understand human language.']\n",
        "```\n",
        "\n",
        "\n",
        "**Word Tokenization**:\n",
        "Divides text into individual words. This method is often used for tasks like text classification and sentiment analysis.\n",
        "\n",
        "```python\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = \"Natural language processing is amazing!\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)\n",
        "# Output: ['Natural', 'language', 'processing', 'is', 'amazing', '!']\n",
        "```\n",
        "\n",
        "\n",
        "**N-grams**:\n",
        "Groups of `n` contiguous tokens. Useful for capturing context and patterns in text.\n",
        "\n",
        "```python\n",
        "from nltk.util import ngrams\n",
        "text = \"Natural language processing is amazing\"\n",
        "tokens = word_tokenize(text)\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "print(bigrams)\n",
        "# Output: [('Natural', 'language'), ('language', 'processing'), ('processing', 'is'), ('is', 'amazing')]\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "**Character Tokenization**:\n",
        "Divides text into individual characters. This is less common but useful for specific tasks like character-level language modeling.\n",
        "\n",
        "```python\n",
        "text = \"NLP\"\n",
        "tokens = list(text)\n",
        "print(tokens)\n",
        "# Output: ['N', 'L', 'P']\n",
        "```"
      ],
      "metadata": {
        "id": "MRFkyK_ZM1iw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming and Lemmatization"
      ],
      "metadata": {
        "id": "zO3ZlCarPpDy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming and lemmatization are text preprocessing techniques used in natural language processing (NLP) to reduce words to their base or root form. This helps in standardizing text and improving the effectiveness of text analysis and machine learning models.\n",
        "\n",
        "**1. Stemming**\n",
        "\n",
        "Stemming reduces a word to its base or root form by removing suffixes. The stem might not be a proper word but a common root that different words share. for example the stem of words \"running,\" \"runner,\" \"ran\" is \"run\"\n",
        "\n",
        "\n",
        "**Popular Stemming Algorithms**:\n",
        "- **Porter Stemmer**: Applies heuristic rules to remove common suffixes.\n",
        "- **Snowball Stemmer**: An improved version of the Porter Stemmer with better support for multiple languages.\n",
        "\n",
        "**Example with Python’s NLTK**:\n",
        "```python\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer() # Create a Porter Stemmer object\n",
        "words = [\"running\", \"runner\", \"ran\", \"easily\", \"fairly\"] # Example words\n",
        "stems = [stemmer.stem(word) for word in words] # Stem the words\n",
        "print(stems)\n",
        "# Output: ['run', 'runner', 'ran', 'easili', 'fairli']\n",
        "```\n",
        "\n",
        "**2. Lemmatization**\n",
        "Lemmatization reduces a word to its base or root form by considering its context and meaning. Unlike stemming, lemmatization returns actual words that exist in the language and often uses a dictionary or lexical database. for example the lemma of words \"running,\" \"runner,\" \"ran\" is \"run\"\n",
        "\n",
        "**Popular Lemmatization Libraries**:\n",
        "- **WordNet Lemmatizer**: Uses the WordNet lexical database to find the lemma of a word.\n",
        "- **spaCy**: Provides efficient lemmatization as part of its NLP pipeline.\n",
        "\n",
        "**Example with Python’s NLTK**:\n",
        "```python\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "```\n",
        "```python\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "lemmatizer = WordNetLemmatizer() #Create a WordNet Lemmatizer object\n",
        "words = [\"running\", \"runner\", \"ran\", \"easily\", \"fairly\"] #Example words\n",
        "lemmas = [lemmatizer.lemmatize(word, pos=wordnet.VERB) for word in words] #Lemmatize the words\n",
        "print(lemmas)\n",
        "# Output: ['run', 'runner', 'run', 'easily', 'fairly']\n",
        "```"
      ],
      "metadata": {
        "id": "BQ6u6J-oPrA1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction in NLP"
      ],
      "metadata": {
        "id": "Z54wAqCxMZmt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature extraction is the process of converting textual data into a set of numerical features. These features are then used as input for machine learning models, aiding in reducing the data's complexity while retaining relevant information needed for analysis. Common feature extraction techniques are:\n",
        "\n",
        "1. **Bag of Words (BoW)**:\n",
        "   - **Description**: Represents text as a bag (multiset) of words, ignoring grammar and word order but keeping track of word frequency.\n",
        "   - **Feature Representation**: Each unique word in the corpus becomes a feature, with the value being the count (or frequency) of the word in the document.\n",
        "   - **Example**: For documents \"cat sat\" and \"cat sat cat\", the feature vector might be `[2, 1]` for \"cat\" and `[1, 1]` for \"sat\".\n",
        "\n",
        "2. **N-grams**:\n",
        "   - **Description**: Represents sequences of `n` words (or characters) as features, capturing word combinations and patterns.\n",
        "   - **Feature Representation**: Features are created for each sequence of `n` words, with their frequencies or occurrences used as feature values.\n",
        "   - **Example**: For the sentence \"The cat sat\", bigrams are \"The cat\", \"cat sat\".\n",
        "\n",
        "3. **Term Frequency-Inverse Document Frequency (TF-IDF)**:\n",
        "   - **Description**: Enhances BoW by weighing the importance of words based on their frequency in a document relative to their frequency across all documents.\n",
        "   - **Feature Representation**: Each word is assigned a TF-IDF score, reflecting its importance in the document relative to the corpus.\n",
        "   - **Example**: Words that appear frequently in a document but rarely in the corpus get higher TF-IDF scores.\n",
        "\n",
        "4. **Word Embeddings**:\n",
        "   - **Description**: Represents words as dense vectors in a continuous vector space, capturing semantic relationships between words.\n",
        "   - **Feature Representation**: Pre-trained embeddings (e.g., Word2Vec, GloVe) provide vector representations for words based on their context and meaning.\n",
        "   - **Example**: The word \"king\" might be represented by a vector `[0.2, -0.1, 0.4, ...]`.\n",
        "\n",
        "\n",
        "5. **Sentence Embeddings**:\n",
        "   - **Description**: Represents entire sentences or documents as dense vectors that capture the meaning of the whole text.\n",
        "   - **Feature Representation**: Models like Universal Sentence Encoder or BERT provide vector representations for sentences or documents.\n",
        "   - **Example**: A sentence like \"The cat sat on the mat\" might be represented by a vector `[0.1, -0.3, 0.2, ...]`."
      ],
      "metadata": {
        "id": "JurduW6MUZDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of Words"
      ],
      "metadata": {
        "id": "3AnsOH4dOD1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Bag of Words model represents a text document as a collection of its words, disregarding the **grammar** and the **context** but keeping track of the frequency of each word. Essentially, it transforms text into a vector of word counts or occurrences.\n",
        "\n",
        "Suppose we have the following two sentences:\n",
        "\n",
        "1. \"programming is fun\"\n",
        "2. \"programming is easy and easy.\"\n",
        "\n",
        "**Step 1: Tokenization**\n",
        "\n",
        "- Sentence 1: `['programming', 'is', 'fun']`\n",
        "- Sentence 2: `['programming', 'is', 'easy', 'and', 'easy']`\n",
        "\n",
        "**Step 2: Vocabulary Building**\n",
        "\n",
        "- Vocabulary: `['and', 'easy', 'fun', 'is', 'programming']`\n",
        "\n",
        "**Step 3: Vectorization**\n",
        "\n",
        "- For Sentence 1: `[0, 0, 1, 1, 1]`\n",
        "- For Sentence 2: `[1, 2, 0, 1, 1]`\n",
        "\n",
        "**Implementation Example:**\n",
        "\n",
        "```python\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "documents = [\n",
        "    \"programming is fun\",\n",
        "    \"programming is easy and easy.\"\n",
        "]\n",
        "vectorizer = CountVectorizer() #Create a Bag of Words model\n",
        "X = vectorizer.fit_transform(documents)\n",
        "features = vectorizer.get_feature_names_out() #Get the feature names (words in the vocabulary)\n",
        "print(\"Feature Names:\", features)\n",
        "print(\"Bag of Words Matrix:\\n\", X.toarray()) #Get the Bag of Words representation\n",
        "```"
      ],
      "metadata": {
        "id": "Kc4Rz1pYOGLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF"
      ],
      "metadata": {
        "id": "5oEuQodLRcUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF (Term Frequency-Inverse Document Frequency) is used in text processing to evaluate the importance of a word in a document relative to a collection of documents, or corpus. It helps in converting text into numerical features that can be used in machine learning algorithms and information retrieval systems. It has two main components:\n",
        "\n",
        "**1. Term Frequency (TF):** The term frequency of a word in a document is the number of times the word appears in the document, normalized by the total number of words in that document.\n",
        "\n",
        "$ \\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d} $\n",
        "\n",
        "**Example**:\n",
        "- Document: \"The cat in the hat.\"\n",
        "- Term Frequency of \"cat\": 1 / 5 = 0.2\n",
        "\n",
        "**2. Inverse Document Frequency (IDF):** The inverse document frequency measures how important a term is by considering how often it appears across all documents in the corpus. Words that appear in many documents are less informative.\n",
        "\n",
        "$ \\text{IDF}(t) = \\log \\left(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing term } t}\\right) $\n",
        "\n",
        "**Example**:\n",
        "- Corpus: 1000 documents\n",
        "- Number of documents containing \"cat\": 50\n",
        "- IDF of \"cat\": $\\log \\left(\\frac{1000}{50}\\right) = \\log (20) \\approx 1.30$\n",
        "\n",
        "**3. TF-IDF Calculation:** TF-IDF is the product of TF and IDF. It gives a numerical value representing the importance of a term in a document relative to the corpus.\n",
        "\n",
        "**TF-IDF of \"cat\": $0.2 \\times 1.30 = 0.26$**\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"The cat in the hat\",\n",
        "    \"The cat is on the mat\",\n",
        "    \"The dog is on the mat\"\n",
        "]\n",
        "vectorizer = TfidfVectorizer() #Create a TF-IDF vectorizer\n",
        "tfidf_matrix = vectorizer.fit_transform(documents) #Fit and transform the documents\n",
        "feature_names = vectorizer.get_feature_names_out() #Get feature names (words)\n",
        "tfidf_array = tfidf_matrix.toarray() #Convert the matrix to an array and print it\n",
        "print(\"Feature Names:\", feature_names)\n",
        "print(\"TF-IDF Matrix:\\n\", tfidf_array)\n",
        "```"
      ],
      "metadata": {
        "id": "mt0nVhwqRfLZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Embedding"
      ],
      "metadata": {
        "id": "0ZhPBTJnTNpl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word embeddings are a type of word representation used in natural language processing (NLP) that map words or phrases into continuous vector spaces. Unlike traditional methods like the Bag of Words (BoW) or TF-IDF, which represent words as discrete tokens or counts, word embeddings provide dense, low-dimensional representations that capture semantic meaning and relationships between words.\n",
        "\n",
        "\n",
        "**Popular Algorithms for Word Embeddings**:\n",
        "\n",
        "- **Word2Vec**: Developed by Google, it uses two main models:\n",
        "  - **Continuous Bag of Words (CBOW)**: Predicts a target word based on its context words.\n",
        "  - **Skip-gram**: Predicts context words given a target word.\n",
        "\n",
        "- **GloVe (Global Vectors for Word Representation)**: Developed by Stanford, it uses matrix factorization on the word co-occurrence matrix to learn embeddings.\n",
        "\n",
        "- **FastText**: An extension of Word2Vec by Facebook, which represents words as bags of character n-grams, allowing for better handling of morphologically rich languages.\n",
        "\n",
        "- **Transformers (e.g., BERT, GPT)**: Modern models like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer) provide context-sensitive embeddings and are used for more advanced NLP tasks.\n",
        "\n",
        "\n",
        "**Example with Word2Vec using Python’s `gensim` library**:\n",
        "\n",
        "```python\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Sample sentences\n",
        "sentences = [\n",
        "    ['the', 'cat', 'sat', 'on', 'the', 'mat'],\n",
        "    ['the', 'dog', 'barked'],\n",
        "    ['the', 'cat', 'is', 'on', 'the', 'roof']\n",
        "]\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)\n",
        "word_vectors = model.wv #Get word vectors\n",
        "vector_cat = word_vectors['cat']\n",
        "vector_dog = word_vectors['dog']\n",
        "\n",
        "print(\"Vector for 'cat':\", vector_cat)\n",
        "print(\"Vector for 'dog':\", vector_dog)\n",
        "```\n",
        "\n",
        "**Example with `transformers` library**:\n",
        "```python\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "# Encode text\n",
        "inputs = tokenizer(\"The bank is by the river bank.\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Extract embeddings\n",
        "last_hidden_states = outputs.last_hidden_state\n",
        "print(last_hidden_states)\n",
        "```\n",
        "\n",
        "**Output**:\n",
        "```\n",
        "tensor([[[-0.0608,  0.0237, -0.0156, ..., -0.0105,  0.0223, -0.0125],\n",
        "         [ 0.0462,  0.0123, -0.0312, ..., -0.0191,  0.0141,  0.0310],\n",
        "         ...\n",
        "         [ 0.0208, -0.0094, -0.0155, ..., -0.0028, -0.0158, -0.0185]]],\n",
        "       grad_fn=<NativeLayerNormBackward>)\n",
        "```"
      ],
      "metadata": {
        "id": "RqTHdi04TP6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentence Embedding"
      ],
      "metadata": {
        "id": "Apu4ER8XkJ-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence embedding is a technique used to convert sentences into fixed-size vectors that capture the semantic meaning of the sentences. There are various methods to obtain sentence embeddings, including traditional methods like averaging word embeddings and modern methods like using pre-trained models from libraries like Hugging Face’s `transformers`.\n",
        "\n",
        "Here are two approaches for generating sentence embeddings:\n",
        "\n",
        "**<h4>1. Using Pre-trained Models from Hugging Face Transformers</h4>**\n",
        "\n",
        "For this approach, you’ll use a pre-trained model like BERT or Sentence-BERT from the Hugging Face `transformers` library.\n",
        "\n",
        "```python\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "def get_sentence_embedding(sentence):\n",
        "    # Tokenize input sentence\n",
        "    inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    \n",
        "    # Get the embeddings of the [CLS] token\n",
        "    cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
        "    return cls_embedding.squeeze().numpy()\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"This is an example sentence.\"\n",
        "embedding = get_sentence_embedding(sentence)\n",
        "print(\"Sentence embedding:\", embedding)\n",
        "```\n",
        "\n",
        "**<h4> 2. Using Averaged Word Embeddings</h4>**\n",
        "\n",
        "This method uses pre-trained word embeddings (e.g., GloVe, FastText) and averages them to get a sentence embedding. Here’s how you can do it with GloVe embeddings:\n",
        "\n",
        "```python\n",
        "from gensim.models import KeyedVectors\n",
        "import numpy as np\n",
        "glove_vectors = KeyedVectors.load_word2vec_format('glove.6B.50d.txt', binary=False, no_header=True)\n",
        "def get_word_embeddings(word):\n",
        "    return glove_vectors[word] if word in glove_vectors else np.zeros(glove_vectors.vector_size)\n",
        "def get_sentence_embedding(sentence):\n",
        "    words = sentence.lower().split()\n",
        "    word_embeddings = [get_word_embeddings(word) for word in words]\n",
        "    sentence_embedding = np.mean(word_embeddings, axis=0)\n",
        "    return sentence_embedding\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"This is an example sentence.\"\n",
        "embedding = get_sentence_embedding(sentence)\n",
        "print(\"Sentence embedding:\", embedding)\n",
        "```\n",
        "\n",
        "<h4> Explanation\n",
        "\n",
        "1. **Hugging Face Transformers**:\n",
        "   - **Model and Tokenizer**: Load a pre-trained BERT model and tokenizer from Hugging Face.\n",
        "   - **Tokenization**: Convert the sentence to tokens that the model understands.\n",
        "   - **Embedding Extraction**: Use the output from the model, specifically the embedding of the `[CLS]` token, which represents the sentence.\n",
        "\n",
        "2. **Averaged Word Embeddings**:\n",
        "   - **GloVe or Similar Embeddings**: Load pre-trained word vectors.\n",
        "   - **Word Embeddings**: Retrieve embeddings for each word in the sentence.\n",
        "   - **Average Embedding**: Compute the average of the word embeddings to get a sentence embedding."
      ],
      "metadata": {
        "id": "jKMoijc2kXen"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLP Applications"
      ],
      "metadata": {
        "id": "FLWa322hZLxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Similarity Analysis"
      ],
      "metadata": {
        "id": "PXL8k7d0bGzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Sample texts\n",
        "texts = [\n",
        "    \"Machine learning is a field of artificial intelligence.\",\n",
        "    \"Artificial intelligence encompasses machine learning and deep learning.\",\n",
        "    \"I love learning about new technologies and artificial intelligence.\",\n",
        "]\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Transform texts into TF-IDF matrices\n",
        "tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Compute cosine similarity between each pair of texts\n",
        "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "# Print cosine similarity matrix\n",
        "print(\"Cosine Similarity Matrix:\")\n",
        "print(cosine_sim)"
      ],
      "metadata": {
        "id": "2Du1gokiiRit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a15c78a-2c62-4f06-93b5-7087686a423b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity Matrix:\n",
            "[[1.         0.40055573 0.20517518]\n",
            " [0.40055573 1.         0.36320897]\n",
            " [0.20517518 0.36320897 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4> Explanation:\n",
        "\n",
        "1. **TF-IDF Vectorizer**: Converts text data into TF-IDF features.\n",
        "2. **Fit and Transform**: Transforms the text data into TF-IDF matrices.\n",
        "3. **Cosine Similarity**: Measures similarity between texts based on their TF-IDF representations.\n",
        "\n",
        "The resulting `cosine_sim` matrix shows the similarity scores between each pair of texts. Each value in the matrix ranges from 0 to 1, where 1 indicates complete similarity and 0 indicates no similarity."
      ],
      "metadata": {
        "id": "xVUDrUKjbMNW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Classification"
      ],
      "metadata": {
        "id": "ZIXoZjuHZqNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn import preprocessing, linear_model, metrics\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "url = 'https://drive.google.com/file/d/1mcTm9p3zqfXZwEmT-iU6f2w9AxGxS6yg/view?usp=sharing'\n",
        "path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n",
        "df = pd.read_csv(path)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Uba5aaY1dLkB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "a16e3ba9-c93c-4d6e-c859-a5fdbe493635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  label\n",
              "0  Stuning even for the non-gamer: This sound tra...      2\n",
              "1  The best soundtrack ever to anything.: I'm rea...      2\n",
              "2  Amazing!: This soundtrack is my favorite music...      2\n",
              "3  Excellent Soundtrack: I truly like this soundt...      2\n",
              "4  Remember, Pull Your Jaw Off The Floor After He...      2"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-59f38b43-6fb8-4322-ac50-9b68175fb9a2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-59f38b43-6fb8-4322-ac50-9b68175fb9a2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-59f38b43-6fb8-4322-ac50-9b68175fb9a2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-59f38b43-6fb8-4322-ac50-9b68175fb9a2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-125f4287-36af-4a50-a4c8-213a84643f62\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-125f4287-36af-4a50-a4c8-213a84643f62')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-125f4287-36af-4a50-a4c8-213a84643f62 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10000,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          \"I Know I'm in the Minority Here, But...: I'm so sorry to be such a spoilsport, but I found this is a terrible book and a worse series. Leaving aside all the wild inaccuracies - one of the main ones being Auel's portrayal of Neandertals as doltish brutes -the heroine is constantly making one earthshaking discovery after another - shampoo, domestication of animals, law of genetics, sex=pregnancy, and, last but not least, tampax. No one else can figure anything out, but there she is, churning out one Nobel-Laureate discovery after another. I'm sure in one of the later books she will be found making a short-wave radio out of a large rock. It became really maddening.\",\n          \"Disappointing: This is the 4th Clavell book I've read, I was more than a little disappointed. I've read them in order so far, Shogun and Tai-pan for me were breathtaking, long books which I sped through. Gai-jin was more of a chore, with not much of ending. I found King Rat dull, I think the story wasn't too interesting. Other customers have commented on how harsh the prison was - too be honest, I thought it would've been worse. The King had none of the charisma of the heroes in the other books, and I found myself not really caring whether or not either him or Marlowe made it. Although a lot was made of their inner feelings, this made me feel that they were both just shallow people.\",\n          \"The writing is good, but the plot needs a lot of work.: I love books that make me laugh, and a friend told me about this book. I paid full price, and I feel somewhat had. Ms. Jensen has a gift for humor, but the plotting involved in this book is horrible. Good humor has to have some element of believability in it, and this book has none whatsoever. With a believable story line, Ms. Jensen could be among the top humor writers out there.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'])"
      ],
      "metadata": {
        "id": "zTUyR6CCdo71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a count vectorizer object\n",
        "count_vect = CountVectorizer()\n",
        "count_vect.fit(df['text'])\n",
        "\n",
        "# transform the training and testing data using count vectorizer object\n",
        "X_train_cv =  count_vect.transform(X_train)\n",
        "X_test_cv =  count_vect.transform(X_test)"
      ],
      "metadata": {
        "id": "iC2UGTQfeKkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word level tf-idf\n",
        "tfidf_vect = TfidfVectorizer()\n",
        "tfidf_vect.fit(df['text'])\n",
        "X_train_tfidf =  tfidf_vect.transform(X_train)\n",
        "X_test_tfidf =  tfidf_vect.transform(X_test)"
      ],
      "metadata": {
        "id": "lMdMlOw8eatL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "def train_model(classifier, X_train, y_train, X_test):\n",
        "    classifier.fit(X_train, y_train)\n",
        "    predictions = classifier.predict(X_test)\n",
        "    return metrics.accuracy_score(predictions, y_test)"
      ],
      "metadata": {
        "id": "t185R2uraagc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Classifier on Count Vectors\n",
        "accuracy = train_model(svm.SVC(), X_train_cv, y_train, X_test_cv)\n",
        "print(\"Count Vectors Accuracy: \", accuracy)\n",
        "\n",
        "# Train Classifier on Word Level TF IDF Vectors\n",
        "accuracy = train_model(svm.SVC(), X_train_tfidf, y_train, X_test_tfidf)\n",
        "print(\"WordLevel TF-IDF Accuracy: \", accuracy)"
      ],
      "metadata": {
        "id": "8LYOuZtWahpE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bc7ff22-69a7-4a6c-8039-a57bf07eae42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count Vectors Accuracy:  0.846\n",
            "WordLevel TF-IDF Accuracy:  0.8692\n"
          ]
        }
      ]
    }
  ]
}