{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n",
        "Data preprocessing is a crucial step in the data analysis and machine learning pipeline. It involves the cleaning and transformation of raw data into a format that is suitable for analysis or input to a machine learning model. The main goals of data preprocessing are to improve the quality of the data and enhance the performance and effectiveness of machine learning models. The choice of preprocessing techniques is influenced by the nature of the data, and different algorithms are applied accordingly to address unique challenges associated with diverse data types."
      ],
      "metadata": {
        "id": "v01JF_t15h5A"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVtxQgfIMt2g"
      },
      "source": [
        "## Data Types"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The columns in a Pandas DataFrame can contain different types of data. Here are some common types you might encounter:\n",
        "\n",
        "1. **Numerical Data:**\n",
        "   - **Discrete:** These are numerical data that have a countable number of distinct values. For example, the **number of cars** in a parking lot or the **number of students in a classroom**.\n",
        "   - **Continuous :** They can take any numeric value within a range and have an infinite number of possible values. Examples include **height, weight, or temperature**.\n",
        "\n",
        "2. **Categorical Data:**\n",
        "   - **Nominal:** These type of data represent categories without any inherent order or ranking. Examples include **gender, color, or types of fruits**.\n",
        "   - **Ordinal:** They have categories with a meaningful order. Examples include socio economic status **(low income,middle income,high income), education level (high school,BS,MS,PhD)**.\n",
        "\n",
        "3. **Datetime :**\n",
        "   - **Datetime (datetime64):** Datetime data, also referred to as timestamp or time series data, represents information related to dates and times.\n",
        "\n",
        "4. **Sparse Data:**\n",
        "    - Sparse data refers to data where a large proportion of the elements have a value of zero. This type of data is common in various fields, such as natural language processing, recommendation systems, and network analysis. An example of sparse data is given below where rows represent users and columns represent movies. Each entry in the dataset indicates whether a user has rated a particular movie.\n",
        "\n",
        "    ```\n",
        "    User       Movie A   Movie B   Movie C   Movie D   Movie E\n",
        "    User 1        4         0         0         0         0\n",
        "    User 2        0         0         0         5         0\n",
        "    User 3        0         0         3         0         0\n",
        "    User 4        0         0         0         0         2\n",
        "    User 5        0         1         0         0         0\n",
        "    ```\n",
        "\n",
        "    - Sparse data often requires specific techniques and algorithms to efficiently handle and analyze, as processing all the zero values can be computationally expensive and may not provide meaningful insights.\n",
        "\n",
        "    - The `scipy.sparse` module provides a variety of sparse matrix types and operations for sparse matrix manipulations. It includes formats such as CSR (Compressed Sparse Row), CSC (Compressed Sparse Column), COO (Coordinate), and others.\n"
      ],
      "metadata": {
        "id": "Jiu5FFoiIjfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "url = 'https://drive.google.com/file/d/19aYZVyCsbKp0UEQl8QQagKyHFmromwQg/view?usp=sharing'\n",
        "path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n",
        "df = pd.read_csv(path)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "H7yHJdX9T9x3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **total_bill (Numeric):** Represents the total bill amount for a meal, usually a float.\n",
        "\n",
        "2. **tip (Numeric):** Represents the tip amount given by the customer, usually a float.\n",
        "\n",
        "3. **sex (Categorical):** Represents the gender of the person paying the bill, often categorized as \"Male\" or \"Female.\"\n",
        "\n",
        "4. **smoker (Categorical):** Indicates whether the party was a smoker or non-smoker, often categorized as \"Yes\" or \"No.\"\n",
        "\n",
        "5. **day (Categorical):** Represents the day of the week when the meal took place, categorized as \"Thur,\" \"Fri,\" \"Sat,\" or \"Sun.\"\n",
        "\n",
        "6. **time (Categorical):** Indicates whether the meal was lunch or dinner.\n",
        "\n",
        "7. **size (Categorical):** Represents the size of the dining party.\n"
      ],
      "metadata": {
        "id": "mYkWEGcnfw-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Missing Values"
      ],
      "metadata": {
        "id": "4r4kMvleWkp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling missing data is a crucial aspect of data cleaning and analysis. In pandas, missing data is typically represented by `NaN` (Not a Number). Here are some common techniques for handling missing data in pandas:\n",
        "\n",
        "### 1. Detecting Missing Data:\n",
        "   - The `isnull()` method can be used to detect missing values in a DataFrame. It returns a DataFrame of the same shape, where each element is `True` or `False` based on whether the corresponding element in the original DataFrame is missing.\n",
        "\n",
        "   ```python\n",
        "   # Detect missing values\n",
        "   missing_values = df.isnull()\n",
        "   number_of_missing_values=df.isnull().sum()\n",
        "   ```\n",
        "\n",
        "### 2. Dropping Missing Values:\n",
        "   - The `dropna()` method can be used to remove rows or columns containing missing values. The `axis` parameter specifies whether to drop rows (`axis=0`) or columns (`axis=1`).\n",
        "\n",
        "   ```python\n",
        "   # Drop rows containing missing values\n",
        "   df_no_missing_rows = df.dropna()\n",
        "   # Drop columns containing missing values\n",
        "   df_no_missing_cols = df.dropna(axis=1)\n",
        "   ```\n",
        "\n",
        "### 3. Filling Missing Values:\n",
        "   - The `fillna()` method can be used to fill missing values with a specific value or with the result of a function.\n",
        "\n",
        "   ```python\n",
        "   # Fill missing values with a specific value\n",
        "   df_filled = df.fillna(0)\n",
        "   # Fill missing values with the mean of each column\n",
        "   df_mean_filled = df.fillna(df.mean())\n",
        "   ```\n"
      ],
      "metadata": {
        "id": "GqPs2uttW7ny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "url = 'https://drive.google.com/file/d/19aYZVyCsbKp0UEQl8QQagKyHFmromwQg/view?usp=sharing'\n",
        "path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n",
        "df = pd.read_csv(path)\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "LzD1bz0BjjKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding Categorical Variables"
      ],
      "metadata": {
        "id": "qcMkOxeaXI3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding categorical  is a crucial step in the data preprocessing phase, especially when working with machine learning models.\n",
        "\n",
        "Most Common Categorical Data Encoding Techniques:\n",
        "\n",
        "1. **Label Encoding:**\n",
        "\n",
        "    - Assigns a unique integer to each category.\n",
        "    - Suitable for ordinal data where there is a natural order among categories.\n",
        "    - Not recommended for nominal data as it may imply misleading relationships.\n",
        "\n",
        "2. **One-Hot Encoding:**\n",
        "\n",
        "    - Creates binary columns for each category (0 or 1).\n",
        "    - Suitable for nominal data without any natural order.\n",
        "    - Increases dimensionality but avoids false ordinal relationships."
      ],
      "metadata": {
        "id": "ux7pOK6gXZoq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Label Encoding"
      ],
      "metadata": {
        "id": "ep_j4HXrsM8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels={'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5, 'six': 6}\n",
        "df['size']=df['size'].map(labels)\n",
        "df"
      ],
      "metadata": {
        "id": "M9sLVVOzswDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One-Hot Encoding"
      ],
      "metadata": {
        "id": "75ipY4SVtEDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform one-hot encoding on the 'category' column\n",
        "one_hot_encoded_df = pd.get_dummies(df,columns=['sex','day','time','smoker'], drop_first=False)\n",
        "one_hot_encoded_df"
      ],
      "metadata": {
        "id": "TcAe0jPhqbku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Scaling"
      ],
      "metadata": {
        "id": "YIGYwtbq_dJO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature scaling is a data preprocessing technique used in machine learning and statistics to scale the values of features ( or attributes) in a dataset to a standard range. The primary goal of feature scaling is to ensure that all the features have similar scales, which can help improve the performance of many machine learning algorithms. It is particularly important when using algorithms that are sensitive to the scale of input features, such as gradient descent-based optimization methods (e.g., in neural networks) and distance-based algorithms (e.g., k-nearest neighbors or support vector machines).\n",
        "\n",
        "Common methods of feature scaling include:\n",
        "\n",
        "1. Min-Max Scaling (Normalization):\n",
        "   - This method scales the feature values to a specific range, typically between 0 and 1.\n",
        "   - The formula for min-max scaling is:\n",
        "     ```\n",
        "     X_normalized = (X - X_min) / (X_max - X_min)\n",
        "     ```\n",
        "   - Here, X is the original feature value, X_normalized is the normalized value, X_min is the minimum value in the feature, and X_max is the maximum value in the feature.\n",
        "\n",
        "    **The function below accepts a Pandas column as a parameter and returns the scaled data:**\n",
        "    ```python\n",
        "    def minmax_scale(column):\n",
        "        min_val = column.min()\n",
        "        max_val = column.max()\n",
        "        scaled_column = (column - min_val) / (max_val - min_val)\n",
        "        return scaled_column\n",
        "    ```\n",
        "   \n",
        "\n",
        "2. Standardization:\n",
        "   - Also known as z-score standardization, scales the feature values to have a mean (average) of 0 and a standard deviation of 1.\n",
        "   - The formula for standardization is:\n",
        "     ```\n",
        "     X_standardized = (X - mean) / standard deviation\n",
        "     ```\n",
        "   - Here, X is the original feature value, X_standardized is the standardized value, mean is the mean of the feature values, and the standard deviation is the standard deviation of the feature values.\n",
        "   \n",
        "    **The function below accepts a Pandas column as a parameter and returns the scaled data:**\n",
        "   ```python\n",
        "    def zscore_standardize(column):\n",
        "        mean_val = column.mean()\n",
        "        std_dev = column.std()\n",
        "        standardized_column = (column - mean_val) / std_dev\n",
        "        return standardized_column\n",
        "   ```\n",
        "\n",
        "3. Robust Scaling:\n",
        "   - Robust scaling is a method that scales the features using the interquartile range (IQR) to make it less sensitive to outliers.\n",
        "   - The formula for robust scaling is:\n",
        "     ```\n",
        "     X_robust = (X - X_median) / (Q3 - Q1)\n",
        "     ```\n",
        "   - Here, X is the original feature value, X_robust is the robust-scaled value, Q1 is the first quartile, and Q3 is the third quartile of the feature values.\n",
        "\n",
        "      ```python\n",
        "      def robust_scale(column):\n",
        "        median_val = column.median()\n",
        "        iqr = column.quantile(0.75) - column.quantile(0.25)\n",
        "        scaled_column = (column - median_val) / iqr\n",
        "        return scaled_column\n",
        "        ```"
      ],
      "metadata": {
        "id": "KJG-hXAnAKHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Min-Max scaling function\n",
        "def minmax_scale(column):\n",
        "    min_val = column.min()\n",
        "    max_val = column.max()\n",
        "    scaled_column = (column - min_val) / (max_val - min_val)\n",
        "    return scaled_column\n",
        "\n",
        "df['total_bill']=minmax_scale(df['total_bill'])\n",
        "df['tip']=minmax_scale(df['tip'])"
      ],
      "metadata": {
        "id": "Do3HuX5llITq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "jRPpQRS_lv71"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4r4kMvleWkp-",
        "qcMkOxeaXI3f",
        "ep_j4HXrsM8C",
        "75ipY4SVtEDh",
        "YIGYwtbq_dJO"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}